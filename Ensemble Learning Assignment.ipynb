{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ae4e11-9526-4959-b840-2fd902af1659",
   "metadata": {},
   "source": [
    "# Ensemble Learning Assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdac98-4575-4b74-8e34-a3792e907b97",
   "metadata": {},
   "source": [
    "## Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
    "\n",
    "### Answer:**Introduction to Ensemble Learning**\n",
    "\n",
    "Ensemble Learning is a **machine learning technique** in which **multiple individual models**, known as **base learners or weak learners**, are combined to form a **single strong predictive model**. Instead of relying on one model, ensemble learning integrates the predictions of several models to achieve **higher accuracy, better generalization, and improved robustness**.\n",
    "\n",
    "The basic philosophy of ensemble learning is that **a collection of models performs better than any single model** when properly combined.\n",
    "\n",
    "### **Key Idea Behind Ensemble Learning**\n",
    "\n",
    "The key idea behind ensemble learning is based on the concept of **collective intelligence**:\n",
    "\n",
    "> **Different models make different errors; combining them reduces overall error.**\n",
    "\n",
    "Each individual model may be weak or moderately accurate, but when their outputs are aggregated, their **individual weaknesses cancel out**, resulting in a more accurate and reliable prediction.\n",
    "\n",
    "This works effectively when:\n",
    "\n",
    "* Models are **diverse**\n",
    "* Errors made by individual models are **uncorrelated**\n",
    "* Predictions are combined using an appropriate strategy\n",
    "\n",
    "### **Mathematical Intuition**\n",
    "\n",
    "Let ( h_1(x), h_2(x), ..., h_n(x) ) be individual learners.\n",
    "\n",
    "The ensemble model ( H(x) ) is formed as:\n",
    "\n",
    "* **Classification:** Majority or weighted voting\n",
    "* **Regression:** Average or weighted average\n",
    "\n",
    "[\n",
    "H(x) = \\frac{1}{n}\\sum_{i=1}^{n} h_i(x)\n",
    "]\n",
    "\n",
    "This aggregation reduces variance and improves stability.\n",
    "\n",
    "### **Working Mechanism of Ensemble Learning**\n",
    "\n",
    "1. A dataset is given as input\n",
    "2. Multiple models are trained using:\n",
    "\n",
    "   * Different subsets of data\n",
    "   * Different algorithms\n",
    "   * Different hyperparameters\n",
    "3. Each model makes a prediction\n",
    "4. Predictions are combined to generate the final output\n",
    "\n",
    "### **Types of Ensemble Learning**\n",
    "\n",
    "#### **a) Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "* Models are trained **independently**\n",
    "* Each model is trained on a random sample of the dataset\n",
    "* Reduces **variance**\n",
    "* Example: **Random Forest**\n",
    "\n",
    "#### **b) Boosting**\n",
    "\n",
    "* Models are trained **sequentially**\n",
    "* Each model focuses on correcting previous errors\n",
    "* Reduces **bias**\n",
    "* Examples: **AdaBoost, Gradient Boosting, XGBoost**\n",
    "\n",
    "#### **c) Stacking**\n",
    "\n",
    "* Uses a **meta-learner** to combine predictions\n",
    "* Learns the best way to combine models\n",
    "* Used in advanced applications\n",
    "\n",
    "### **Why Ensemble Learning Is Effective**\n",
    "\n",
    "* **Bias reduction:** Boosting improves weak learners\n",
    "* **Variance reduction:** Bagging stabilizes predictions\n",
    "* **Improved generalization**\n",
    "* **Robust to noise**\n",
    "* **Handles complex patterns**\n",
    "\n",
    "\n",
    "### **Real-World Example**\n",
    "\n",
    "In medical diagnosis:\n",
    "\n",
    "* One doctor may misdiagnose a patient\n",
    "* A panel of doctors discussing the case leads to a better diagnosis\n",
    "\n",
    "Similarly, ensemble learning combines multiple models to make more accurate predictions.\n",
    "\n",
    "### **Advantages of Ensemble Learning**\n",
    "\n",
    "* Higher prediction accuracy\n",
    "* Reduced overfitting\n",
    "* More stable predictions\n",
    "* Better performance on complex datasets\n",
    "\n",
    "### **Limitations of Ensemble Learning**\n",
    "\n",
    "* Higher computational cost\n",
    "* Increased model complexity\n",
    "* Reduced interpretability\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Ensemble learning is a powerful and widely used approach in machine learning that leverages the **strength of multiple models** to overcome the limitations of individual learners. The key idea is that **combining diverse models leads to superior performance**, making ensemble techniques fundamental in modern machine learning systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f41a3-9e8d-4ec1-b30e-2bbd53ff1986",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between Bagging and Boosting?\n",
    "\n",
    "### Answer:**Definition:**\n",
    "Bagging is an ensemble technique where multiple models are trained **independently** on different **random samples (with replacement)** of the training dataset. The final prediction is obtained by **averaging** (regression) or **majority voting** (classification).\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "* Models are trained **in parallel**\n",
    "* Reduces **variance**\n",
    "* Works well with **unstable models** like decision trees\n",
    "* Does not focus on difficult samples\n",
    "\n",
    "**Example:** Random Forest\n",
    "\n",
    " **Boosting**\n",
    "\n",
    "**Definition:**\n",
    "Boosting is an ensemble technique where models are trained **sequentially**, and each new model focuses on **correcting the errors** made by the previous models. Misclassified samples are given **higher importance** in subsequent training.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "* Models are trained **sequentially**\n",
    "* Reduces **bias**\n",
    "* Focuses on **hard-to-predict samples**\n",
    "* Can overfit if not regularized\n",
    "\n",
    "**Examples:** AdaBoost, Gradient Boosting, XGBoost, CatBoost\n",
    "\n",
    "### **Key Differences Between Bagging and Boosting**\n",
    "\n",
    "| Feature             | Bagging                    | Boosting                                  |\n",
    "| ------------------- | -------------------------- | ----------------------------------------- |\n",
    "| Training Style      | Parallel                   | Sequential                                |\n",
    "| Data Sampling       | Bootstrap sampling         | Uses entire dataset with weighted samples |\n",
    "| Focus               | All samples equally        | Misclassified samples get higher weight   |\n",
    "| Error Reduction     | Reduces variance           | Reduces bias                              |\n",
    "| Overfitting         | Reduces overfitting        | May overfit if not regularized            |\n",
    "| Model Dependency    | Independent models         | Dependent models                          |\n",
    "| Computational Speed | Faster (parallel)          | Slower (sequential)                       |\n",
    "| Noise Sensitivity   | Less sensitive             | More sensitive to noisy data              |\n",
    "| Final Prediction    | Simple averaging or voting | Weighted combination                      |\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "**Bagging:**\n",
    "\n",
    "* Does not reduce bias\n",
    "* Requires many models\n",
    "\n",
    "**Boosting:**\n",
    "\n",
    "* Sensitive to noise\n",
    "* Computationally expensive\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Bagging and Boosting are powerful ensemble methods with different objectives. **Bagging reduces variance by training independent models**, while **Boosting reduces bias by sequentially correcting errors**. Choosing between them depends on the **nature of the data and the learning problem**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105b191-f592-4ccb-bcd2-0af50070b4f2",
   "metadata": {},
   "source": [
    "## Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
    "\n",
    "### Answer:Bootstrap sampling is a **statistical resampling technique** used extensively in **bagging (Bootstrap Aggregating)** methods such as **Random Forest**. It allows the creation of multiple training datasets from a single original dataset, enabling the training of diverse models that improve overall performance.\n",
    "\n",
    "### **Definition of Bootstrap Sampling**\n",
    "\n",
    "Bootstrap sampling involves **randomly selecting samples from the original dataset with replacement**. Each bootstrap sample has the **same size as the original dataset**, but due to replacement:\n",
    "\n",
    "* Some observations appear multiple times\n",
    "* Some observations may not appear at all\n",
    "\n",
    "This randomness introduces **diversity** among training datasets.\n",
    "\n",
    "### **How Bootstrap Sampling Works**\n",
    "\n",
    "1. Start with a dataset of size (N)\n",
    "2. Randomly select (N) samples **with replacement**\n",
    "3. Repeat the process multiple times to create multiple bootstrap datasets\n",
    "4. Train a separate model on each bootstrap sample\n",
    "\n",
    "### **Role of Bootstrap Sampling in Bagging**\n",
    "\n",
    "Bootstrap sampling plays a **central role** in bagging by:\n",
    "\n",
    "* Creating **different versions of the training data**\n",
    "* Allowing models to be trained **independently**\n",
    "* Introducing **diversity among base learners**\n",
    "\n",
    "This diversity is critical for ensemble performance.\n",
    "\n",
    "### **Bootstrap Sampling in Random Forest**\n",
    "\n",
    "Random Forest uses bootstrap sampling in the following ways:\n",
    "\n",
    "1. **Data Sampling:**\n",
    "   Each decision tree is trained on a different bootstrap sample of the data.\n",
    "\n",
    "2. **Out-of-Bag (OOB) Samples:**\n",
    "   Approximately **63%** of the data is used in each bootstrap sample.\n",
    "   The remaining **37%** acts as **out-of-bag data**, which is used for:\n",
    "\n",
    "   * Model validation\n",
    "   * Error estimation without a separate validation set\n",
    "\n",
    "3. **Variance Reduction:**\n",
    "   Since trees see different data, their errors are less correlated.\n",
    "\n",
    "### **Why Bootstrap Sampling Improves Performance**\n",
    "\n",
    "* **Reduces Variance:**\n",
    "  Averaging predictions from trees trained on different samples stabilizes the model.\n",
    "* **Prevents Overfitting:**\n",
    "  Individual trees may overfit, but their average prediction generalizes better.\n",
    "* **Improves Robustness:**\n",
    "  The model becomes less sensitive to noise and outliers.\n",
    "\n",
    "### **Mathematical Intuition**\n",
    "\n",
    "If each tree has high variance but low bias, averaging their outputs reduces the overall variance:\n",
    "\n",
    "[\n",
    "\\text{Var}(\\text{Ensemble}) = \\frac{1}{n}\\text{Var}(\\text{Single Tree})\n",
    "]\n",
    "\n",
    "### **Real-World Analogy**\n",
    "\n",
    "Imagine preparing for an exam by practicing from **different question sets** created by randomly sampling questions from a large pool. Each practice set improves understanding, and combining learning from all sets gives better results.\n",
    "\n",
    "### **Advantages of Bootstrap Sampling in Bagging**\n",
    "\n",
    "* Creates model diversity\n",
    "* Enables parallel training\n",
    "* Improves generalization\n",
    "* Provides out-of-bag error estimation\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "* Some data points may never be used in training\n",
    "* Increased computational cost\n",
    "* Less effective if base models are already stable\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Bootstrap sampling is a **core mechanism** behind bagging methods like Random Forest. By generating multiple training datasets through sampling with replacement, it ensures **diversity among base learners**, reduces variance, and significantly improves model performance and reliability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd5c82-b8e1-43f2-9a7d-bfec841b092d",
   "metadata": {},
   "source": [
    "## Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
    "\n",
    "### **Answer:** Out-of-Bag (OOB) samples are an important concept used in **bagging-based ensemble methods**, especially in **Random Forest**. When a model uses **bootstrap sampling**, each decision tree is trained on a random subset of the training data selected **with replacement**. Because of this sampling method, not all data points are used to train every tree. On average, about **63% of the data points** are selected for training a tree, while the remaining **37% are left out**. These unused data points are known as **Out-of-Bag samples**.\n",
    "\n",
    "OOB samples play a crucial role in **model evaluation**. Since these samples are not used during the training of a particular tree, they can be treated as **unseen data** for that tree. Each data point is OOB for multiple trees in the forest. To evaluate the model, predictions for a data point are made using only those trees for which the data point was OOB. These predictions are then combined using **majority voting in classification** or **averaging in regression**.\n",
    "\n",
    "The **OOB score** is calculated by comparing the predicted values obtained from OOB samples with their actual target values. In classification problems, the OOB score represents the **classification accuracy**, while in regression problems it is measured using **mean squared error or R² score**. This score provides an **unbiased estimate of the model’s performance** without requiring a separate validation dataset.\n",
    "\n",
    "Thus, OOB score is a highly efficient evaluation technique because it **saves data**, **reduces computational cost**, and provides a reliable estimate of generalization error. For this reason, OOB evaluation is widely used in ensemble models like Random Forest.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18ff2-d612-476a-bb79-80d3596761ab",
   "metadata": {},
   "source": [
    "## Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
    "\n",
    "### **Answer:**\n",
    "\n",
    "Feature importance analysis is a technique used in machine learning to determine **how much each input feature contributes to the model’s predictions**. Both **Decision Trees** and **Random Forests** provide built-in methods to measure feature importance, but the **reliability and stability** of these importance scores differ significantly between the two models.\n",
    "\n",
    "In a **single Decision Tree**, feature importance is calculated based on how much a feature reduces impurity (such as Gini Index or entropy) when it is used to split the data. Features that are used near the **top of the tree** generally receive higher importance because they affect a larger portion of the data. However, since a single decision tree is trained on the **entire dataset**, it is highly sensitive to **noise and small changes in data**. As a result, the feature importance obtained from a single tree can be **unstable and biased**, often favoring features with more levels or continuous values.\n",
    "\n",
    "On the other hand, **Random Forest** computes feature importance by **averaging the importance scores across many decision trees**. Each tree in the forest is trained on a different **bootstrap sample** of the data and considers a random subset of features at each split. This randomness reduces overfitting and ensures that no single feature dominates all trees. The final feature importance score in Random Forest is therefore **more robust, reliable, and less sensitive to noise** compared to a single decision tree.\n",
    "\n",
    "Another important difference is that Random Forest can also estimate feature importance using the **Out-of-Bag (OOB) permutation method**, where the values of a feature are randomly shuffled to observe how much the model’s accuracy decreases. A larger drop in accuracy indicates higher importance. This method provides a **more realistic measure of feature influence**, which is not available in a single decision tree.\n",
    "\n",
    "In summary, while a single Decision Tree provides **simple and interpretable feature importance**, it is often unreliable due to overfitting. Random Forest, by combining multiple trees and averaging their importance scores, offers a **more stable and accurate feature importance analysis**, making it preferable for real-world applications.\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Aspect              | Decision Tree                      | Random Forest                           |\n",
    "| ------------------- | ---------------------------------- | --------------------------------------- |\n",
    "| Basis of Importance | Impurity reduction                 | Average impurity reduction across trees |\n",
    "| Stability           | Low (data sensitive)               | High (robust and stable)                |\n",
    "| Overfitting         | High                               | Low                                     |\n",
    "| Bias                | Can be biased toward some features | Reduced bias                            |\n",
    "| Reliability         | Less reliable                      | More reliable                           |\n",
    "| Advanced Methods    | Not available                      | Permutation / OOB importance            |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b220086-0ed7-4cca-950e-a173274d0dd9",
   "metadata": {},
   "source": [
    "## Question 6: Write a Python program to:\n",
    "● Load the Breast Cancer dataset using\n",
    "sklearn.datasets.load_breast_cancer()\n",
    "● Train a Random Forest Classifier\n",
    "● Print the top 5 most important features based on feature importance scores.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1912e1c-70c4-4283-8790-7c80adda511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "\n",
    "# Print top 5 important features\n",
    "top_5 = importance.sort_values(by='Importance', ascending=False).head(5)\n",
    "print(top_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de08c66b-c3d0-4fd0-9308-3dc2b52ae811",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to:\n",
    "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "● Evaluate its accuracy and compare with a single Decision Tree\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26014dc-5b46-4d74-868e-2bdfa7f04a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "bag_pred = bagging.predict(X_test)\n",
    "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
    "\n",
    "# Print accuracies\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f85840b-87a4-4c09-90e2-f405192284a4",
   "metadata": {},
   "source": [
    "## Question 8: Write a Python program to:\n",
    "● Train a Random Forest Classifier\n",
    "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "● Print the best parameters and final accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3d041f-11ad-404c-960c-3320c759cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred = best_rf.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Final Accuracy:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bd9e6-a68e-411a-9fe4-df59699e65bc",
   "metadata": {},
   "source": [
    "## Question 9: Write a Python program to:\n",
    "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
    "Housing dataset\n",
    "● Compare their Mean Squared Errors (MSE)\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e541af12-9cb9-44a8-90be-e13ac9960edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.2568603365368378\n",
      "Random Forest Regressor MSE: 0.25638991335459355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Bagging Regressor\n",
    "bagging = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "bag_pred = bagging.predict(X_test)\n",
    "bag_mse = mean_squared_error(y_test, bag_pred)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# Print MSE values\n",
    "print(\"Bagging Regressor MSE:\", bag_mse)\n",
    "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9907d-ed06-450f-b3b1-2ad9b46b29cf",
   "metadata": {},
   "source": [
    "## Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "Explain your step-by-step approach to:\n",
    "● Choose between Bagging or Boosting\n",
    "● Handle overfitting\n",
    "● Select base models\n",
    "● Evaluate performance using cross-validation\n",
    "● Justify how ensemble learning improves decision-making in this real-world\n",
    "context.\n",
    "\n",
    "### Answer:**1️.Choosing between Bagging or Boosting**\n",
    "\n",
    "* **Bagging** is preferred when the model suffers from **high variance** (overfitting).\n",
    "* **Boosting** is preferred when the model has **high bias** and struggles to learn complex patterns.\n",
    "* In loan default prediction, data is complex and noisy → **Boosting (e.g., AdaBoost / Gradient Boosting)** is often more effective.\n",
    "\n",
    "#### **2️.Handling Overfitting**\n",
    "\n",
    "* Use **ensemble methods** to combine multiple weak learners.\n",
    "* Apply **cross-validation** to check generalization.\n",
    "* Limit tree depth (`max_depth`) and use regularization parameters.\n",
    "\n",
    "#### **3️.Selecting Base Models**\n",
    "\n",
    "* **Decision Trees** are chosen as base learners because:\n",
    "\n",
    "  * They capture non-linear relationships\n",
    "  * Work well with mixed numerical data\n",
    "  * Easily boosted or bagged\n",
    "\n",
    "#### **4️.Evaluating Performance using Cross-Validation**\n",
    "\n",
    "* Use **K-Fold Cross-Validation** to:\n",
    "\n",
    "  * Reduce dependency on a single train-test split\n",
    "  * Ensure stable and reliable accuracy\n",
    "\n",
    "\n",
    "#### **5️.Why Ensemble Learning Improves Decision-Making**\n",
    "\n",
    "* Combines multiple models → more robust predictions\n",
    "\n",
    "* Reduces risk of wrong loan approval/rejection\n",
    "\n",
    "* Improves financial safety and customer trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f18388-d2d2-44cd-a0d5-145258768533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.91 0.9  0.91 0.87 0.92]\n",
      "Average Accuracy: 0.9020000000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Simulated loan default dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Base model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Bagging Classifier\n",
    "bagging = BaggingClassifier(\n",
    "    dt,\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(bagging, X, y, cv=5)\n",
    "\n",
    "# Output\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Average Accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4ca18-9305-4e68-a458-1ca9580161b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
