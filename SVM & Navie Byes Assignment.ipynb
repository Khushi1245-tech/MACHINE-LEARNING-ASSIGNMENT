{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42df4d4e-d36b-4ab7-bc98-b6c523bc7ce6",
   "metadata": {},
   "source": [
    "# SVM & Navie Byes Assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d5a06-4195-446d-9abd-0fcdda849f88",
   "metadata": {},
   "source": [
    "## Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
    "\n",
    "### **Answer:** Information Gain is an important concept used in **Decision Tree algorithms** to decide **which feature should be selected for splitting the data at each node**. It is based on the idea of **reducing uncertainty** in the dataset. In decision trees, uncertainty is measured using **entropy**, and Information Gain tells us **how much entropy is reduced after a dataset is split on a particular feature**.\n",
    "\n",
    "Entropy is a measure of randomness or impurity in a dataset. If all data points belong to the same class, entropy is zero, meaning there is no uncertainty. However, if the data points are evenly distributed among different classes, entropy is high. Information Gain is calculated as the **difference between the entropy before the split and the weighted entropy after the split**.\n",
    "\n",
    "Mathematically, entropy is defined as:\n",
    "[\n",
    "Entropy(S) = -\\sum p_i \\log_2(p_i)\n",
    "]\n",
    "where ( p_i ) is the probability of class ( i ) in dataset ( S ).\n",
    "\n",
    "Information Gain is calculated using the formula:\n",
    "[\n",
    "IG(S, A) = Entropy(S) - \\sum \\frac{|S_v|}{|S|} Entropy(S_v)\n",
    "]\n",
    "where ( A ) is the feature used for splitting and ( S_v ) represents the subsets formed after the split.\n",
    "\n",
    "In a Decision Tree, Information Gain is used during the **tree construction process**. At each node, the algorithm calculates the Information Gain for all possible features and selects the feature with the **highest Information Gain** for splitting. This ensures that the chosen feature provides the **maximum reduction in uncertainty**, leading to purer child nodes and a more effective tree structure.\n",
    "\n",
    "The main advantage of using Information Gain is that it helps build **compact and meaningful decision trees** by choosing features that best separate the classes. However, one limitation of Information Gain is that it tends to favor features with **many distinct values**, which can sometimes lead to overfitting. To address this issue, alternative measures such as **Gain Ratio** are used.\n",
    "\n",
    "In conclusion, Information Gain is a fundamental criterion in Decision Trees that measures the **effectiveness of a feature in classifying data**. By selecting splits that maximize Information Gain, decision trees become more accurate, efficient, and interpretable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32f30a-a062-45bb-b854-9b66ac942519",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between Gini Impurity and Entropy? Hint: Directly compares the two main impurity measures, highlighting strengths,weaknesses, and appropriate use cases.\n",
    "\n",
    "### **Answer:**\n",
    "\n",
    "Gini Impurity and Entropy are two commonly used **impurity measures** in **Decision Tree algorithms** to evaluate the quality of a split. Both measures quantify how **mixed or impure** a dataset is, and they help the decision tree decide **which feature should be chosen at each node**. Although they serve the same purpose, they differ in calculation, interpretation, and practical use.\n",
    "\n",
    "**Entropy** is derived from **information theory** and measures the amount of **uncertainty or randomness** in a dataset. If all instances in a node belong to a single class, entropy is zero, indicating complete purity. As class distribution becomes more uniform, entropy increases. Entropy is commonly used in the **ID3 and C4.5 algorithms** and focuses on maximizing **Information Gain** during splitting.\n",
    "\n",
    "On the other hand, **Gini Impurity** measures the probability of **incorrectly classifying a randomly chosen data point** if it were labeled according to the class distribution in that node. Like entropy, Gini impurity is zero when all data points belong to one class. However, it increases as the class distribution becomes more mixed. Gini impurity is widely used in the **CART (Classification and Regression Trees)** algorithm.\n",
    "\n",
    "From a computational perspective, **Gini impurity is faster to calculate** because it does not involve logarithmic computations, while entropy requires logarithms, making it slightly more expensive computationally. Due to this reason, Gini impurity is often preferred in large datasets and real-time applications.\n",
    "\n",
    "In terms of behavior, **entropy tends to create more balanced splits**, while **Gini impurity may isolate the most frequent class more quickly**. However, in practical scenarios, both measures often produce **very similar trees**, and the difference in performance is usually minimal.\n",
    "\n",
    "In conclusion, both Gini impurity and entropy are effective impurity measures for decision trees. Entropy is more theoretically grounded in information theory, while Gini impurity is computationally efficient. The choice between them depends on **algorithm preference, dataset size, and computational constraints** rather than major differences in predictive performance.\n",
    "\n",
    "### **Comparison Table:**\n",
    "\n",
    "| Aspect             | Gini Impurity                 | Entropy                   |\n",
    "| ------------------ | ----------------------------- | ------------------------- |\n",
    "| Concept            | Misclassification probability | Measure of uncertainty    |\n",
    "| Formula            | (1 - \\sum p_i^2)              | (-\\sum p_i \\log_2 p_i)    |\n",
    "| Value Range        | 0 to 0.5 (binary)             | 0 to 1 (binary)           |\n",
    "| Computational Cost | Lower (no logs)               | Higher (log calculations) |\n",
    "| Algorithms         | CART                          | ID3, C4.5                 |\n",
    "| Split Behavior     | Favors dominant class         | Produces balanced splits  |\n",
    "| Practical Use      | Faster, large datasets        | More theoretical clarity  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbcd97-921e-4a80-a453-04bb65e4ae7d",
   "metadata": {},
   "source": [
    "## Question 3:What is Pre-Pruning in Decision Trees?\n",
    "\n",
    "### Answer:Pre-pruning in Decision Trees is a technique used to stop the growth of a decision tree early in order to prevent overfitting. Instead of allowing the tree to grow fully, pre-pruning applies certain stopping criteria during tree construction so that unnecessary or insignificant splits are avoided.\n",
    "\n",
    "In pre-pruning, the tree stops splitting a node when conditions such as maximum tree depth, minimum number of samples required to split a node, minimum information gain, or minimum samples in a leaf node are met. By restricting further splits, the model becomes simpler, faster, and more generalizable to unseen data.\n",
    "\n",
    "In summary, pre-pruning helps control model complexity and reduces overfitting, but if applied too aggressively, it may lead to underfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18177f-6518-4fe4-a465-20839b05d6fd",
   "metadata": {},
   "source": [
    "## Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_. (Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8607aed7-0411-41a7-b6ce-dab865211f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "sepal length (cm): 0.0133\n",
      "sepal width (cm): 0.0000\n",
      "petal length (cm): 0.5641\n",
      "petal width (cm): 0.4226\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 2: Load dataset (Iris dataset as example)\n",
    "iris = load_iris()\n",
    "X = iris.data   # Features\n",
    "y = iris.target # Labels\n",
    "\n",
    "# Step 3: Create Decision Tree Classifier using Gini Impurity\n",
    "dtree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# Step 4: Train the model\n",
    "dtree.fit(X, y)\n",
    "\n",
    "# Step 5: Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, dtree.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ad996-cf88-475d-bec2-94e263b833dc",
   "metadata": {},
   "source": [
    "## Question 5: What is a Support Vector Machine (SVM)?\n",
    "\n",
    "### Answer:A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. It works by finding the best boundary or hyperplane that separates data points of different classes. The support vectors are the closest points to this boundary, and SVM maximizes the margin between the classes for better accuracy. If the data is not linearly separable, SVM can use kernel functions to transform it into a higher-dimensional space where a separation is possible. SVM is especially useful for high-dimensional and complex datasets because it focuses only on the critical points that define the boundary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1a1ef-c314-4fb9-9e3a-08652e790f95",
   "metadata": {},
   "source": [
    "## Question 6: What is the Kernel Trick in SVM?\n",
    "\n",
    "### Answer:The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle data that is not linearly separable. Sometimes, you cannot draw a straight line (or hyperplane) to separate two classes in the original feature space. The kernel trick transforms the data into a higher-dimensional space where the classes can be separated linearly, without explicitly computing the transformation for every point.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* It allows SVM to find complex boundaries without heavy computation.\n",
    "\n",
    "* Common kernels include:\n",
    "\n",
    "* Linear Kernel – for linearly separable data\n",
    "\n",
    "* Polynomial Kernel – for curved boundaries\n",
    "\n",
    "* RBF (Radial Basis Function) Kernel – for complex patterns\n",
    "\n",
    "* The trick helps SVM work efficiently on non-linear data.\n",
    "\n",
    "**Example:**\n",
    "Imagine red and blue dots forming a circle inside another circle. In 2D, you cannot separate them with a straight line. Using the kernel trick, SVM can project the points into 3D, where a plane can separate the classes perfectly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940dc9c-f307-4a43-9fb0-3e780ce70474",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
    "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
    "on the same dataset.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb65ef09-d98e-4bef-aead-f4f595c14d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear Kernel SVM: 0.9815\n",
      "Accuracy of RBF Kernel SVM: 0.7593\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Step 3: Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Create SVM classifiers\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Step 5: Train the models\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Step 7: Calculate and compare accuracies\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Accuracy of Linear Kernel SVM: {accuracy_linear:.4f}\")\n",
    "print(f\"Accuracy of RBF Kernel SVM: {accuracy_rbf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bcd08-8bf4-40a6-a4e2-c22391fb0dbb",
   "metadata": {},
   "source": [
    "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "\n",
    "## Answer:**Naïve Bayes Classifier:**\n",
    "\n",
    "The **Naïve Bayes classifier** is a **probabilistic machine learning algorithm** used for classification tasks. It is based on **Bayes’ Theorem**, which calculates the probability of a class given certain features.\n",
    "\n",
    "It is called **“Naïve”** because it **assumes that all features are independent of each other**, even if in reality they might be related. This simplification makes the algorithm fast and efficient, especially for **large datasets**, like in spam detection, text classification, or sentiment analysis.\n",
    "\n",
    "**In simple words:**\n",
    "Imagine trying to guess if an email is spam based on words it contains. Naïve Bayes assumes each word contributes **independently** to the chance of being spam, ignoring any relationship between words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc578b5f-43df-4f7c-af76-62dd8166f4e6",
   "metadata": {},
   "source": [
    "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
    "\n",
    "### Answer:**Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes:**\n",
    "\n",
    "| Variant                     | Data Type                 | Key Idea                                                      | Typical Use Case                                                                  |\n",
    "| --------------------------- | ------------------------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **Gaussian Naïve Bayes**    | Continuous numerical data | Assumes **features follow a Gaussian (normal) distribution**  | Predicting outcomes with real-valued data, e.g., height, weight, temperature      |\n",
    "| **Multinomial Naïve Bayes** | Discrete count data       | Uses **feature counts** (how many times a feature occurs)     | Text classification, document classification, spam detection (word counts)        |\n",
    "| **Bernoulli Naïve Bayes**   | Binary/Boolean data       | Considers **whether a feature is present or absent** (0 or 1) | Text classification with **binary features**, e.g., whether a word appears or not |\n",
    "\n",
    "**Summary in simple words:**\n",
    "\n",
    "* **Gaussian NB** → Good for **numbers** (continuous features).\n",
    "* **Multinomial NB** → Good for **counts** (how many times something occurs).\n",
    "* **Bernoulli NB** → Good for **yes/no data** (presence or absence of something).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809800b-5741-4dce-874c-9adbf9306535",
   "metadata": {},
   "source": [
    "## Question 10: Breast Cancer Dataset\n",
    "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
    "dataset and evaluate accuracy.\n",
    "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
    "sklearn.datasets.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a820bd84-1b76-4403-aa55-fbbb28135073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: 0.9415\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Step 3: Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Create Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Step 5: Train the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Gaussian Naive Bayes on Breast Cancer dataset: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
