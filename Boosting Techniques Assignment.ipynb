{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e284fe-9cb8-4433-9359-5b8933f3ba6f",
   "metadata": {},
   "source": [
    "# **Boosting Techniques Assignment**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07402a9d-d665-4edb-aa81-be851eddbacf",
   "metadata": {},
   "source": [
    "## Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
    "\n",
    "**Answer:** **Definition of Boosting:**\n",
    "Boosting is an **ensemble learning technique** in machine learning that combines multiple **weak learners** to create a **strong learner**. A weak learner is a model that performs slightly better than random guessing (e.g., a shallow decision tree). Boosting works **sequentially**, where each new model focuses on correcting the errors made by the previous models.\n",
    "\n",
    "**How Boosting Works:**\n",
    "The process of boosting can be explained in steps:\n",
    "\n",
    "1. **Train a weak learner** on the training data.\n",
    "2. **Evaluate errors**: Identify which samples were misclassified by this model.\n",
    "3. **Assign higher weights** to the misclassified samples so that the next learner focuses more on these difficult cases.\n",
    "4. **Train the next weak learner** using the updated weights.\n",
    "5. **Combine all learners**: After multiple iterations, the predictions of all weak learners are combined (usually weighted) to produce the final strong model.\n",
    "\n",
    "Mathematically, in methods like **AdaBoost**:\n",
    "\n",
    "* Each weak learner (h_t(x)) is assigned a weight (\\alpha_t) based on its accuracy.\n",
    "* The final prediction is:\n",
    "  [\n",
    "  H(x) = \\text{sign} \\Big(\\sum_{t=1}^T \\alpha_t h_t(x)\\Big)\n",
    "  ]\n",
    "\n",
    "**How Boosting Improves Weak Learners:**\n",
    "\n",
    "* **Focus on Hard Cases:** Each weak learner in the sequence pays more attention to the samples misclassified by previous learners.\n",
    "* **Error Reduction:** By combining multiple weak models, the overall error decreases, producing a strong learner.\n",
    "* **Weighted Voting:** Predictions are combined in a weighted manner so that more accurate learners contribute more to the final prediction.\n",
    "* **Bias-Variance Reduction:** Boosting reduces **bias** by focusing on misclassified points and also can reduce **variance** as multiple models are combined.\n",
    "\n",
    "**Popular Boosting Algorithms:**\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting)** – Adjusts weights of misclassified samples.\n",
    "2. **Gradient Boosting** – Builds models to predict the **residual errors** of previous models.\n",
    "3. **XGBoost / LightGBM / CatBoost** – Advanced boosting methods used in competitions for higher speed and accuracy.\n",
    "\n",
    "**Key Advantages of Boosting:**\n",
    "\n",
    "* Converts weak learners into a strong learner.\n",
    "* Works well on complex datasets.\n",
    "* Can significantly improve prediction accuracy.\n",
    "\n",
    "**Example:**\n",
    "Imagine a class of students (weak learners) who each know a little about a subject. Individually, they may not perform well, but if each one focuses on what the others got wrong and then shares their knowledge, together they achieve excellent results.\n",
    "\n",
    "**Summary:**\n",
    "Boosting is a **sequential ensemble technique** that **improves weak learners** by focusing on their errors, weighting difficult samples, and combining multiple weak models into a **strong and accurate model**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9514573-2b46-47ed-b517-1748dba195c6",
   "metadata": {},
   "source": [
    "## Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
    "\n",
    "**Answer:** **1. AdaBoost (Adaptive Boosting):**\n",
    "\n",
    "* **Training Approach:** AdaBoost trains weak learners **sequentially**, **focusing on misclassified samples** from previous learners.\n",
    "* **Weighting:** Each training sample is assigned a weight. Misclassified samples get **higher weights**, so the next learner focuses more on them.\n",
    "* **Error Correction:** The algorithm reduces **classification errors** by adjusting sample weights.\n",
    "* **Combination:** Each learner is given a weight in the final prediction based on its accuracy.\n",
    "\n",
    "**Summary:** AdaBoost modifies **data weights** to improve the next learner.\n",
    "\n",
    "**2. Gradient Boosting:**\n",
    "\n",
    "* **Training Approach:** Gradient Boosting trains weak learners **sequentially**, but each new learner is trained to **predict the residual errors (gradients) of the previous model**.\n",
    "* **Error Correction:** Instead of weighting samples, Gradient Boosting fits learners to the **errors made by the previous model** (residuals).\n",
    "* **Optimization:** It uses **gradient descent** to minimize a **loss function** (e.g., mean squared error for regression).\n",
    "* **Combination:** Predictions are combined **additively** to produce the final output.\n",
    "\n",
    "**Summary:** Gradient Boosting modifies **predictions** to reduce the loss, rather than weighting data points.\n",
    "\n",
    "### **Key Difference Table:**\n",
    "\n",
    "| Feature            | AdaBoost                        | Gradient Boosting                   |\n",
    "| ------------------ | ------------------------------- | ----------------------------------- |\n",
    "| Focus              | Misclassified samples           | Residual errors of previous model   |\n",
    "| Sample weighting   | Yes, adjusts weights of samples | No, uses gradients of loss function |\n",
    "| Loss optimization  | Implicit (via error weighting)  | Explicit (via gradient descent)     |\n",
    "| Common Use         | Classification                  | Regression & Classification         |\n",
    "| Combination Method | Weighted vote                   | Additive prediction                 |\n",
    "\n",
    "\n",
    "**Example Analogy:**\n",
    "\n",
    "* AdaBoost → “Pay more attention to the mistakes.”\n",
    "* Gradient Boosting → “Learn to fix the mistakes step by step.”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe354c-4d69-423e-bbcb-56bcdab003a2",
   "metadata": {},
   "source": [
    "## **Question 3: How does regularization help in XGBoost?**\n",
    "\n",
    "**Answer:** **Introduction to XGBoost Regularization:**\n",
    "XGBoost (Extreme Gradient Boosting) is an advanced boosting algorithm that includes **regularization techniques** to prevent **overfitting**. Regularization in XGBoost helps the model **generalize better** to unseen data by **penalizing complexity** in the trees.\n",
    "\n",
    "**How Regularization Works in XGBoost:**\n",
    "\n",
    "XGBoost adds a **regularization term** to the loss function:\n",
    "\n",
    "[\n",
    "\\text{Objective} = \\text{Loss (training error)} + \\text{Regularization term (model complexity)}\n",
    "]\n",
    "\n",
    "* The **loss function** measures how well the model fits the data (e.g., mean squared error).\n",
    "* The **regularization term** penalizes complex trees by considering:\n",
    "\n",
    "  * **Number of leaves in a tree ((T))**\n",
    "  * **Leaf weights ((w))**\n",
    "  * **L1 (Lasso) and L2 (Ridge) penalties**\n",
    "\n",
    "[\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum w_j^2\n",
    "]\n",
    "Where:\n",
    "\n",
    "* ( \\gamma ) → penalty for each leaf (controls tree depth)\n",
    "* ( \\lambda ) → L2 regularization on leaf weights (controls overfitting)\n",
    "\n",
    "**Benefits of Regularization in XGBoost:**\n",
    "\n",
    "1. **Reduces Overfitting:**\n",
    "\n",
    "   * Prevents trees from becoming too deep or complex.\n",
    "   * Avoids memorizing training data, improving **generalization**.\n",
    "\n",
    "2. **Controls Model Complexity:**\n",
    "\n",
    "   * Encourages simpler trees that are more robust to noise.\n",
    "\n",
    "3. **Improves Prediction Accuracy:**\n",
    "\n",
    "   * By balancing **fit vs. complexity**, the model performs better on unseen data.\n",
    "\n",
    "4. **Supports Sparse Data Handling:**\n",
    "\n",
    "   * L1 regularization can effectively ignore irrelevant features.\n",
    "\n",
    "\n",
    "**Intuitive Analogy:**\n",
    "Imagine trying to memorize answers for an exam:\n",
    "\n",
    "* Without regularization → you memorize every detail (overfit)\n",
    "* With regularization → you learn the **important patterns only**, ignoring noise (better generalization)\n",
    "\n",
    "**Summary:**\n",
    "Regularization in XGBoost **penalizes overly complex trees** through L1/L2 penalties and leaf penalties, which **reduces overfitting, improves generalization, and makes the model more accurate**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff47bab-98f0-46bf-92e3-06a917e6d095",
   "metadata": {},
   "source": [
    "## Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
    "\n",
    "**Answer:** **Introduction to CatBoost:**\n",
    "CatBoost is a **gradient boosting algorithm** specifically designed to handle **categorical features** efficiently. Unlike other boosting algorithms (like XGBoost or LightGBM), CatBoost can process categorical data **without the need for extensive preprocessing** like one-hot encoding.\n",
    "\n",
    "**How CatBoost Handles Categorical Data Efficiently:**\n",
    "\n",
    "1. **Ordered Target Encoding (Permutation-driven):**\n",
    "\n",
    "   * CatBoost converts categorical features into numbers using a technique called **“ordered target statistics”**.\n",
    "   * It calculates the **mean target value** for each category but in a way that **prevents data leakage** by only using previous examples in the permutation.\n",
    "   * This ensures the model **does not overfit** to the training data.\n",
    "\n",
    "2. **No One-Hot Encoding Needed:**\n",
    "\n",
    "   * Traditional algorithms require converting categorical variables into one-hot vectors, which can **explode dimensionality** if there are many categories.\n",
    "   * CatBoost works **directly with categorical data**, making it **memory-efficient and faster**.\n",
    "\n",
    "3. **Combats Overfitting:**\n",
    "\n",
    "   * CatBoost’s approach ensures **robust handling of high-cardinality features** (features with many unique values) without overfitting.\n",
    "\n",
    "4. **Efficient and Fast Training:**\n",
    "\n",
    "   * Its **symmetric tree structure** and efficient handling of categorical features allow **faster training** compared to other gradient boosting methods.\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "| Advantage                            | Explanation                                                 |\n",
    "| ------------------------------------ | ----------------------------------------------------------- |\n",
    "| Handles Categorical Data Directly    | No one-hot encoding needed, avoids high dimensionality      |\n",
    "| Reduces Overfitting                  | Uses ordered target statistics to prevent data leakage      |\n",
    "| Efficient Computation                | Faster training on large datasets                           |\n",
    "| Works with High-Cardinality Features | Can handle features with many unique categories effectively |\n",
    "\n",
    "\n",
    "**Intuitive Analogy:**\n",
    "Imagine you have a list of cities as a feature. Instead of turning each city into a separate column (one-hot), CatBoost **assigns meaningful numbers based on past outcomes**, making the model smarter and faster.\n",
    "\n",
    "**Answer Summary:**\n",
    "CatBoost is efficient for categorical data because it **directly handles categories using ordered target encoding**, **avoids one-hot encoding**, **prevents overfitting**, and **enables faster, memory-efficient training**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d3af9-f54c-48d7-973d-e12bd481f1b1",
   "metadata": {},
   "source": [
    "## Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
    "\n",
    "**Answer:** **Introduction:**\n",
    "Boosting and bagging are both ensemble methods, but they work differently:\n",
    "\n",
    "* **Bagging** (e.g., Random Forest) reduces **variance** by averaging predictions of many independent models.\n",
    "* **Boosting** (e.g., AdaBoost, Gradient Boosting, XGBoost, CatBoost) reduces **bias** by sequentially improving weak learners.\n",
    "\n",
    "**Boosting is preferred when:**\n",
    "\n",
    "* High predictive accuracy is required.\n",
    "* Data has **complex patterns** that simple models struggle to capture.\n",
    "* Avoiding underfitting is more important than overfitting (boosting can overfit if not regularized, but it often performs better than bagging).\n",
    "\n",
    "\n",
    "**Real-World Applications of Boosting:**\n",
    "\n",
    "| Application Area                                    | Why Boosting is Preferred                                                                                                                   |\n",
    "| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Finance – Credit Scoring & Fraud Detection**      | Boosting focuses on difficult-to-predict cases, e.g., rare fraud transactions, improving accuracy in classification.                        |\n",
    "| **Healthcare – Disease Prediction**                 | Boosting models like XGBoost can detect subtle patterns in patient data for predicting diseases (cancer, heart disease) with high accuracy. |\n",
    "| **Marketing – Customer Churn Prediction**           | Sequential error correction allows boosting to predict which customers are likely to leave, even when patterns are complex.                 |\n",
    "| **E-commerce – Recommendation Systems**             | Boosting captures intricate patterns in user behavior better than bagging models.                                                           |\n",
    "| **Competitions – Kaggle & Data Science Challenges** | Boosting algorithms (XGBoost, CatBoost, LightGBM) consistently outperform bagging methods in predictive modeling contests.                  |\n",
    "| **Natural Language Processing (NLP)**               | Text classification, sentiment analysis, and spam detection benefit from boosting’s focus on misclassified examples.                        |\n",
    "| **Insurance – Risk Assessment**                     | Boosting captures nuanced patterns in claims data for better risk prediction.                                                               |\n",
    "\n",
    "\n",
    "**Why Boosting Over Bagging in These Cases:**\n",
    "\n",
    "* **High Bias Problem:** Boosting reduces bias effectively, while bagging mainly reduces variance.\n",
    "* **Focus on Hard Cases:** Boosting gives more weight to misclassified samples, which is crucial in fraud detection, churn prediction, and rare disease prediction.\n",
    "* **Better Accuracy:** Boosting often achieves higher predictive performance when the dataset has complex, non-linear relationships.\n",
    "\n",
    "**Intuitive Analogy:**\n",
    "\n",
    "* Bagging → “Ask 100 people independently and average their opinion.”\n",
    "* Boosting → “Ask 1 person at a time, teach them from previous mistakes, and then combine their refined opinions.”\n",
    "\n",
    "**Summary:**\n",
    "Boosting techniques are preferred in **finance, healthcare, marketing, e-commerce, NLP, insurance, and competitive modeling** because they **focus on difficult-to-predict cases**, **reduce bias**, and **achieve higher accuracy** than bagging methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a26a32f-1645-47f4-a25f-d50cee0179ac",
   "metadata": {},
   "source": [
    "## Datasets:\n",
    "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
    "## Question 6: Write a Python program to:\n",
    "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
    "● Print the model accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2428f-62dd-453b-a1e0-02e5a8a4e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff1d99-ea58-43e1-9e16-b0854dcb9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Base estimator (MANDATORY in new versions)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# AdaBoost classifier\n",
    "model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ad83e-3b1a-4c5a-afa5-b47eac0bebd0",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to:\n",
    "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
    "● Evaluate performance using R-squared score\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182f849-f780-4802-b6b7-626a202278d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate using R-squared score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print R-squared score\n",
    "print(\"R-squared Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f4274-c9a6-40d7-8cb4-a003f7c1796c",
   "metadata": {},
   "source": [
    "## Question 8: Write a Python program to:\n",
    "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
    "● Tune the learning rate using GridSearchCV\n",
    "● Print the best parameters and accuracy\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb11f5-73b4-4b7d-9cfc-9bc34372bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d073b-2c57-4f7c-afd8-2bc8b4cc17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "xgb = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Grid for learning rate\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "grid = GridSearchCV(\n",
    "    xgb,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Output\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882d951-44d8-451f-97d0-62c125929bd4",
   "metadata": {},
   "source": [
    "## Question 9: Write a Python program to:\n",
    "● Train a CatBoost Classifier\n",
    "● Plot the confusion matrix using seaborn\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268e8ff-a739-4270-a663-5cc305ae038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533f7166-ca3c-4348-9631-1e1f22acbe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9736842105263158\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAHFCAYAAACn7hC1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCSElEQVR4nO3de5yN5f7/8fcyhzUHMxMjc8AwGHLIWROlISFJ2b4d5NA4VI7VsMNPdpmym8nUlnZCKadKqFBUopQUMsROwq4MOpicD01jmJnr94eHtVtmsNaybmusXs/9uB+7dd3Xuu/PvVrTfOZzXfd124wxRgAAAB4o5+sAAADA5YtEAgAAeIxEAgAAeIxEAgAAeIxEAgAAeIxEAgAAeIxEAgAAeIxEAgAAeIxEAgAAeIxEogz45ptv1K9fPyUmJiokJETly5dXs2bNlJWVpUOHDll67k2bNiklJUVRUVGy2WyaNGmS189hs9mUnp7u9eNeyKxZs2Sz2WSz2fTZZ5+V2G+MUe3atWWz2dS2bVuPzjFlyhTNmjXLrfd89tln54zpUjl27JieeuoptWjRQpGRkbLb7apRo4b69++vr7/+2u3j/frrr0pPT9fmzZtL7EtPT3f8e7DZbCpXrpzi4uJ0yy236Msvv/TC1Vyc88V+Pjt37tSwYcNUp04dhYaGKiwsTA0aNNA//vEP/fLLL45+ffv2VY0aNbwbtBt27dolm81W4ns6f/58NWjQQKGhobLZbNq8ebPj3xXgjkBfB/BXN336dA0ZMkR169bVyJEjVb9+fZ06dUobNmzQtGnTtHbtWi1atMiy8/fv3195eXmaN2+eKlSoYMl/8NauXauqVat6/biuioiI0KuvvloiWVi1apV+/PFHRUREeHzsKVOmqFKlSurbt6/L72nWrJnWrl2r+vXre3zei/Hjjz+qY8eO2rdvnwYNGqQnnnhC5cuX165du7RgwQI1b95cR44cUVRUlMvH/PXXX/XEE0+oRo0aatKkSal9li1bpqioKBUXF2vPnj3KyspS27Zt9dVXX6lZs2Zeujr3uRL72ZYuXaoePXqoUqVKGjZsmJo2bSqbzaYtW7ZoxowZev/997Vp0yZrA3dRXFyc1q5dq1q1ajna9u/frz59+ujmm2/WlClTZLfbVadOHd133326+eabfRgtLksGPrNmzRoTEBBgbr75ZnPixIkS+wsKCsy7775raQyBgYFm8ODBlp7DV2bOnGkkmfvuu8+Ehoaao0ePOu3v3bu3adWqlWnQoIFJSUnx6BzuvPfkyZPm1KlTHp3HWwoLC83VV19tIiMjzZYtW0rt88EHH5i8vDy3jpudnW0kmZkzZ5bYN27cOCPJ7N+/36n9xx9/NJLMmDFj3DqXt50v9tLs3LnThIeHm6ZNm5ojR46U2F9cXGzeeecdx+vU1FRTvXp1L0XrHV988YWRZObPn2/pedz9HuHyRCLhQ7feeqsJDAw0e/bscal/UVGRmTBhgqlbt64JDg42V155penTp4/56aefnPqlpKSYBg0amPXr15vrr7/ehIaGmsTERJOZmWmKioqMMf/7JXv2Zsz//sN/tjPvycnJcbR98sknJiUlxVSsWNGEhISYatWqme7duzv9B0SSGTdunNOxtmzZYm677TZzxRVXGLvdbho3bmxmzZrl1OfTTz81kszcuXPNo48+auLi4kxERIRp37692b59+wU/rzPxfvLJJyY0NNRMmzbNse/IkSMmNDTUTJ8+vdRkID093VxzzTWmQoUKJiIiwjRt2tS88sorpri42NGnevXqJT6/M78wzsQ+Z84cM2LECBMfH29sNpvZtm2bY9+nn35qjDFm//79pmrVqqZVq1bm5MmTjuNv3brVhIWFmd69e1/wWl319ttvG0kmMzPTpf7ff/+96du3r6ldu7YJDQ018fHx5tZbbzXffPONo8+Z6zl7O/Pv/FyJxIEDB4wk8/jjjzu179692/Tq1ctceeWVJjg42Fx11VXm2WefdXx3zzh48KAZPHiwiY+PN0FBQSYxMdE8+uijJZLyBQsWmGuuucZERkY6fhb69evnUuylGTZsmJFk1q5d69JnWFoiMXnyZNOmTRtz5ZVXmrCwMNOwYUMzYcIEp3//xhjz9ddfmy5dujg+i7i4OHPLLbc4/cyf7/qMMSYnJ8cpUUpNTS1xvWe+/+f62Z83b5659tprTVhYmAkPDzcdO3Y0X3/9dYnrDA8PN998843p0KGDKV++vLn22mtd+oxweWOOhI8UFRVp5cqVat68uapVq+bSewYPHqzRo0erQ4cOeu+99zR+/HgtW7ZMrVu31oEDB5z65ubmqlevXurdu7fee+89de7cWWPGjNHrr78uSerSpYvWrl0rSbrjjju0du1ax2tX7dq1S126dFFwcLBmzJihZcuW6emnn1Z4eLhOnjx5zvft2LFDrVu31tatW/Xvf/9bCxcuVP369dW3b19lZWWV6P/oo49q9+7deuWVV/Tyyy/r+++/V9euXVVUVORSnJGRkbrjjjs0Y8YMR9ubb76pcuXK6e677z7ntQ0cOFALFizQwoUL1b17dz344IMaP368o8+iRYtUs2ZNNW3a1PH5nT0MNWbMGO3Zs0fTpk3TkiVLVLly5RLnqlSpkubNm6fs7GyNHj1akvTHH3/ozjvvVEJCgqZNm+bSdbpi+fLlkqRu3bq51P/XX39VdHS0nn76aS1btkwvvviiAgMDlZycrB07dkg6PVQzc+ZMSdI//vEPx2dx3333OR2rqKhIhYWFOnnypH744QcNHTpUdrtdd9xxh6PP/v371bp1ay1fvlzjx4/Xe++9p5tuukmPPPKIhg0b5uh34sQJtWvXTnPmzNGIESP0/vvvq3fv3srKylL37t0d/dauXau7775bNWvW1Lx58/T+++/r8ccfV2FhoVuxn/0ZxsTE6Nprr3XpMyzNjz/+qJ49e+q1117T0qVLNWDAAD3zzDMaOHCgo09eXp46dOig3377TS+++KJWrFihSZMmKSEhQcePH3fp+krz2GOP6cUXX5QkZWRkaO3atZoyZco5+2dkZOiee+5R/fr1tWDBAr322ms6fvy42rRpo++++86p78mTJ3Xbbbfpxhtv1LvvvqsnnnjC488IlxFfZzJ/Vbm5uUaS6dGjh0v9t23bZiSZIUOGOLV/9dVXRpJ59NFHHW0pKSlGkvnqq6+c+tavX9906tTJqU2SGTp0qFObqxWJM3/dbt68+byx66y/8Hr06GHsdnuJSkznzp1NWFiYo1x85q/FW265xanfggULXPqL8Ey82dnZjmN9++23xhhjWrZsafr27WuMufDwRFFRkTl16pR58sknTXR0tFNV4lzvPXO+G2644Zz7zlQkzpgwYYKRZBYtWmRSU1NNaGio01/+3nDzzTcbSaUOpbmisLDQnDx50iQlJZnhw4c72l0Z2jh7i4yMNAsXLnTq+//+3/8r9bs7ePBgY7PZzI4dO4wxxkybNs1IMgsWLHDqd+YzXL58uTHGmGeffdZIKnUIwpXYSxMSEuLWX9oXGto48/2aM2eOCQgIMIcOHTLGGLNhwwYjySxevPic73Xl+s6uSBjzv+/gW2+95dT37J/9PXv2mMDAQPPggw869Tt+/LiJjY01d911l9N1SjIzZsw4ZyzwT1QkLhOffvqpJJWY1HfNNdeoXr16+uSTT5zaY2Njdc011zi1NWrUSLt37/ZaTE2aNFFwcLAeeOABzZ49Wzt37nTpfStXrlT79u1LVGL69u2rP/74o0Rl5LbbbnN63ahRI0ly61pSUlJUq1YtzZgxQ1u2bFF2drb69+9/3hhvuukmRUVFKSAgQEFBQXr88cd18OBB7du3z+Xz/t///Z/LfUeOHKkuXbronnvu0ezZs/XCCy/o6quvvuD7CgsLnTZjjMvndOXYGRkZql+/voKDgxUYGKjg4GB9//332rZtm1vH+vjjj5Wdna3169dr6dKluummm9SjRw+nKs7KlStVv379Et/dvn37yhijlStXOvqFh4c7VTPO9JPk+Hlo2bKlJOmuu+7SggULnO6m8KVNmzbptttuU3R0tOP7de+996qoqEj//e9/JUm1a9dWhQoVNHr0aE2bNq3EX/+S9df30UcfqbCwUPfee6/TdywkJEQpKSml3nnkznce/oFEwkcqVaqksLAw5eTkuNT/4MGDkk7PwD5bfHy8Y/8Z0dHRJfrZ7Xbl5+d7EG3patWqpY8//liVK1fW0KFDVatWLdWqVUvPP//8ed938ODBc17Hmf1/dva12O12SXLrWmw2m/r166fXX39d06ZNU506ddSmTZtS+65fv14dO3aUdPqumi+//FLZ2dkaO3as2+ct7TrPF2Pfvn114sQJxcbGqk+fPhd8z65duxQUFOS0rVq16pz9ExISJMnl792IESP02GOPqVu3blqyZIm++uorZWdnq3Hjxm5/lxo3bqwWLVqoZcuW6tKli9566y3Vrl1bQ4cOdfRx9btx8OBBxcbGlrhVsXLlygoMDHT0u+GGG7R48WLHL8OqVauqYcOGevPNN92K/c8SEhJc/vxKs2fPHrVp00a//PKLnn/+ea1evVrZ2dmO4YYzn2tUVJRWrVqlJk2a6NFHH1WDBg0UHx+vcePG6dSpU5Zd35/99ttvkk4nLGd/z+bPn19iSDUsLEyRkZFeOTcuHyQSPhIQEKD27dtr48aN+vnnny/Y/8wv071795bY9+uvv6pSpUpeiy0kJESSVFBQ4NR+9n80JKlNmzZasmSJjh49qnXr1qlVq1ZKS0vTvHnzznn86Ojoc16HJK9ey5/17dtXBw4c0LRp09SvX79z9ps3b56CgoK0dOlS3XXXXWrdurVatGjh0TnduSd/7969Gjp0qJo0aaKDBw/qkUceueB74uPjlZ2d7bQ1b978nP07deokSVq8eLFLMb3++uu69957lZGRoU6dOumaa65RixYtSv0uuKtcuXJq0KCB9u7d66jyuPrdiI6O1m+//Vai+rJv3z4VFhY6fYduv/12ffLJJzp69Kg+++wzVa1aVT179nR7TtAZnTp10m+//aZ169Z59P7FixcrLy9PCxcuVO/evXX99derRYsWCg4OLtH36quv1rx583Tw4EFt3rxZd999t5588kn961//suz6/uzM5/j222+X+J5lZ2frq6++curPGhR/TSQSPjRmzBgZY3T//feXOjnx1KlTWrJkiSTpxhtvlCTHZMkzsrOztW3bNrVv395rcZ1ZS+Kbb75xaj8TS2kCAgKUnJzs+KvqfIsatW/fXitXrnT8cjhjzpw5CgsLu6hJbOdTpUoVjRw5Ul27dlVqauo5+9lsNgUGBiogIMDRlp+fr9dee61EX29VeYqKinTPPffIZrPpww8/VGZmpl544QUtXLjwvO8LDg5WixYtnLbzrYtx++236+qrr1ZmZqa+/fbbUvt89NFH+uOPPySd/izOVIDOeP/990uU0D2pEhUVFWnLli2y2+2Ov2Lbt2+v7777rsT3Z86cObLZbGrXrp2j3++//14iIZozZ45j/9nsdrtSUlI0YcIESXKs8+Bu7MOHD1d4eLiGDBmio0ePlthvjDnv2i9nftn++XM1xmj69OnnfU/jxo313HPP6Yorrij15+tc13cxOnXqpMDAQP34448lvmdnNoAFqXyoVatWmjp1qoYMGaLmzZtr8ODBatCggU6dOqVNmzbp5ZdfVsOGDdW1a1fVrVtXDzzwgF544QWVK1dOnTt31q5du/TYY4+pWrVqGj58uNfiuuWWW1SxYkUNGDBATz75pAIDAzVr1iz99NNPTv2mTZumlStXqkuXLkpISNCJEyccd0bcdNNN5zz+uHHjtHTpUrVr106PP/64KlasqDfeeEPvv/++srKy3FoIyV1PP/30Bft06dJFEydOVM+ePfXAAw/o4MGDevbZZ0v8QpX+9xfj/PnzVbNmTYWEhLg0r+Fs48aN0+rVq7V8+XLFxsbq73//u1atWqUBAwaoadOmSkxMdPuYpQkICNCiRYvUsWNHtWrVSoMHD1a7du0UHh6u3bt36+2339aSJUt0+PBhSdKtt96qWbNm6aqrrlKjRo20ceNGPfPMMyUWGKtVq5ZCQ0P1xhtvqF69eipfvrzi4+MdQxKStHHjRse/299++00zZszQ9u3bNXz4cEcVbPjw4ZozZ466dOmiJ598UtWrV9f777+vKVOmaPDgwapTp44k6d5779WLL76o1NRU7dq1S1dffbW++OILZWRk6JZbbnF8/x5//HH9/PPPat++vapWraojR47o+eefV1BQkFJSUlyO/c8SExM1b9483X333WrSpIljQSpJ+u677zRjxgwZY/S3v/2t1Pd36NBBwcHBuueeezRq1CidOHFCU6dOdXzmZyxdulRTpkxRt27dVLNmTRljtHDhQh05ckQdOnRw+fouRo0aNfTkk09q7Nix2rlzp26++WZVqFBBv/32m9avX6/w8HDuzAB3bZQFmzdvNqmpqSYhIcEEBwc7Frt5/PHHzb59+xz9zqwjUadOHRMUFGQqVapkevfufc51JM5W2uxxlXLXhjHGrF+/3rRu3dqEh4ebKlWqmHHjxplXXnnF6a6NtWvXmr/97W+mevXqxm63m+joaJOSkmLee++9EucobR2Jrl27mqioKBMcHGwaN25cYtb8uWaWlzYLvTR/vmvjfEq782LGjBmmbt26xm63m5o1a5rMzEzz6quvllhHY9euXaZjx44mIiKi1HUkzo79z/vO3LWxfPlyU65cuRKf0cGDB01CQoJp2bKlKSgoOO81uOvIkSNm/PjxplmzZqZ8+fImKCjIJCQkmN69e5svv/zS0e/w4cNmwIABpnLlyiYsLMxcf/31ZvXq1SYlJaXEZ/bmm2+aq666ygQFBZW6jsSft4oVK5rk5GQzY8aMEutD7N692/Ts2dNER0eboKAgU7duXfPMM8+Uuo7EoEGDTFxcnAkMDDTVq1c3Y8aMcbojZenSpaZz586mSpUqJjg42FSuXNnccsstZvXq1S7Ffj4//vijGTJkiKldu7ax2+0mNDTU1K9f34wYMcLpO1Laz92SJUtM48aNTUhIiKlSpYoZOXKk+fDDD52+F9u3bzf33HOPqVWrlgkNDTVRUVHmmmuucVpvxZXru5i7Ns5YvHixadeunYmMjDR2u91Ur17d3HHHHebjjz92us7w8PALfm7wPzZjvDjFGwAA/KUwRwIAAHiMRAIAAHiMRAIAAHiMRAIAAD9Uo0YN2Wy2EtuZReCMMUpPT1d8fLxCQ0PVtm1bbd261e3zkEgAAOCHsrOztXfvXse2YsUKSdKdd94pScrKytLEiRM1efJkZWdnKzY2Vh06dHA8FM5V3LUBAMBfQFpampYuXarvv/9e0umVcdPS0hxPHS4oKFBMTIwmTJjg9CTaC6EiAQDAZaKgoEDHjh1z2s5+nEFpTp48qddff139+/eXzWZTTk6OcnNzHc8Vkv63OuqaNWvciskvV7bs/upGX4cAlEmv92nm6xCAMics2PpnhIQ2HeaV44y+vVKJ1UTHjRun9PT0875v8eLFOnLkiOMJubm5uZKkmJgYp34xMTFuPyXaLxMJAAD80ZgxYzRixAinttKW7z/bq6++qs6dO5dY+v3sB60ZY9x++BqJBAAAVrN5ZyaB3W53KXH4s927d+vjjz92eghgbGyspNOVibi4OEf7vn37SlQpLoQ5EgAAWM1m887mgZkzZ6py5crq0qWLoy0xMVGxsbGOOzmk0/MoVq1apdatW7t1fCoSAABYzUsVCXcVFxdr5syZSk1NVWDg/37l22w2paWlKSMjQ0lJSUpKSlJGRobCwsLUs2dPt85BIgEAgJ/6+OOPtWfPHvXv37/EvlGjRik/P19DhgzR4cOHlZycrOXLlysiIsKtc/jlOhLctQGUjrs2gJIuyV0bLUdcuJML8rMneuU43kRFAgAAq/loaONS8N8rAwAAlqMiAQCA1Ty84+JyQCIBAIDVGNoAAAAoiYoEAABWY2gDAAB4jKENAACAkqhIAABgNYY2AACAx/x4aINEAgAAq/lxRcJ/UyQAAGA5KhIAAFiNoQ0AAOAxP04k/PfKAACA5ahIAABgtXL+O9mSRAIAAKsxtAEAAFASFQkAAKzmx+tIkEgAAGA1hjYAAABKoiIBAIDVGNoAAAAe8+OhDRIJAACs5scVCf9NkQAAgOWoSAAAYDWGNgAAgMcY2gAAACiJigQAAFZjaAMAAHiMoQ0AAICSqEgAAGA1hjYAAIDH/DiR8N8rAwAAlqMiAQCA1fx4siWJBAAAVvPjoQ0SCQAArObHFQn/TZEAAIDlqEgAAGA1hjYAAIDHGNoAAAAoiYoEAAAWs1GRAAAAnrLZbF7Z3PXLL7+od+/eio6OVlhYmJo0aaKNGzc69htjlJ6ervj4eIWGhqpt27baunWrW+cgkQAAwA8dPnxY1113nYKCgvThhx/qu+++07/+9S9dccUVjj5ZWVmaOHGiJk+erOzsbMXGxqpDhw46fvy4y+dhaAMAAKv5YGRjwoQJqlatmmbOnOloq1GjhuOfjTGaNGmSxo4dq+7du0uSZs+erZiYGM2dO1cDBw506TxUJAAAsJgvhjbee+89tWjRQnfeeacqV66spk2bavr06Y79OTk5ys3NVceOHR1tdrtdKSkpWrNmjcvnIZEAAOAyUVBQoGPHjjltBQUFpfbduXOnpk6dqqSkJH300UcaNGiQHnroIc2ZM0eSlJubK0mKiYlxel9MTIxjnytIJAAAsJi3KhKZmZmKiopy2jIzM0s9Z3FxsZo1a6aMjAw1bdpUAwcO1P3336+pU6eWiO3PjDFuVT9IJAAAsJi3EokxY8bo6NGjTtuYMWNKPWdcXJzq16/v1FavXj3t2bNHkhQbGytJJaoP+/btK1GlOB8SCQAALOatRMJutysyMtJps9vtpZ7zuuuu044dO5za/vvf/6p69eqSpMTERMXGxmrFihWO/SdPntSqVavUunVrl6+NuzYAAPBDw4cPV+vWrZWRkaG77rpL69ev18svv6yXX35Z0unkJi0tTRkZGUpKSlJSUpIyMjIUFhamnj17unweEgkAAKzmg9s/W7ZsqUWLFmnMmDF68sknlZiYqEmTJqlXr16OPqNGjVJ+fr6GDBmiw4cPKzk5WcuXL1dERITL57EZY4wVF+BL3V/deOFOwF/Q632a+ToEoMwJC7b+t/wVvV73ynGOvNHbK8fxJuZIAAAAjzG0AQCAxfz5oV0kEgAAWMyfEwmGNgAAgMeoSAAAYDF/rkiQSAAAYDX/zSMY2gAAAJ6jIgEAgMUY2gAAAB4jkQAAAB7z50SCORIAAMBjVCQAALCa/xYkSCQAALAaQxsAAACloCIBAIDF/LkiQSIBAIDF/DmRYGgDAAB4jIoEAAAW8+eKBIkEAABW8988gqENAADgOSoSAABYjKENAADgMRIJAADgMX9OJJgjAQAAPEZFAgAAq/lvQYJEAgAAqzG0AQAAUAoqEvCq7o1i1btlFS399jfN+OpnSVJy9SvU8apKqlUpXJEhgRqx6DvtOpTv40iBS+vVV17Syo9XaFfOTtlDQtS4cVM9PPzvqpFY09eh4RKgIgG4oHalMHW4qpJ2HfzDqT0kqJy2/5an17N/9lFkgO99vSFbd/foqTlvzNfUl2eoqKhQgwfep/w//rjwm3HZs9lsXtnKIioS8IqQwHJKa5uoqV/s1h1N4pz2rfrhkCTpyvLBvggNKBNenPaK0+v08Zlqn9Ja3323Vc1btPRRVMDF82ki8fPPP2vq1Klas2aNcnNzZbPZFBMTo9atW2vQoEGqVq2aL8ODG+5vnaCNPx3VN78eL5FIACjp99+PS5KioqJ8HAkuhbJaTfAGnw1tfPHFF6pXr54WLVqkxo0b695771Xv3r3VuHFjLV68WA0aNNCXX37pq/DghutqVlDN6DC9vuEXX4cCXBaMMfrXM0+rabPmqp1Ux9fh4FKweWkrg3xWkRg+fLjuu+8+Pffcc+fcn5aWpuzs7PMep6CgQAUFBU5tRadOKiCIMvqlEB0epAHXVtOTy77XqSLj63CAy8LTT43X9//doZmz5/o6FOCi+awi8e2332rQoEHn3D9w4EB9++23FzxOZmamoqKinLb/fjDTm6HiPGpVCtMVoUF65vZ6eqtfM73Vr5kaxkXolgaV9Va/ZipXRjNowFeezhivVZ+t1PRX5ygmNtbX4eASYbKlBeLi4rRmzRrVrVu31P1r165VXNyFx9rHjBmjESNGOLX1mbvVKzHiwr759bjSFjp/3sPa1NDPR09o8Te5KqZIAUg6PZwxIWO8Vq78WNNnzFGVqlV9HRIuobKaBHiDzxKJRx55RIMGDdLGjRvVoUMHxcTEyGazKTc3VytWrNArr7yiSZMmXfA4drtddrvdqY1hjUvnxKli7Tl8wrmtsFi/nyh0tJcPDlCl8sGqGBYkSaoSFSJJOpJ/SkfyCy9twICPZD71pD78YKmee/5FhYeH68CB/ZKk8uUjFBIS4uPoYDU/ziN8l0gMGTJE0dHReu655/TSSy+pqKhIkhQQEKDmzZtrzpw5uuuuu3wVHryoZfUr9OANNRyv/37j6QV45n/9q+Zv2uujqIBL6635b0qS7u9/r1P7E+MzdFu37r4ICfAKmzHG58XnU6dO6cCBA5KkSpUqKSgo6KKO1/3Vjd4IC/A7r/dp5usQgDInLNj6ckHSyGVeOc73z9zsleN4U5lYkCooKMil+RAAAFyO/HlogyWyAQCAx8pERQIAAH/GXRsAAMBjfpxHMLQBAAA8RyIBAIDFypWzeWVzR3p6eomVMWP/tJqqMUbp6emKj49XaGio2rZtq61b3V/QkUQCAACL2Wze2dzVoEED7d2717Ft2bLFsS8rK0sTJ07U5MmTlZ2drdjYWHXo0EHHjx936xwkEgAA+KnAwEDFxsY6tiuvvFLS6WrEpEmTNHbsWHXv3l0NGzbU7Nmz9ccff2juXPceJkciAQCAxbz10K6CggIdO3bMaTv7Cdh/9v333ys+Pl6JiYnq0aOHdu7cKUnKyclRbm6uOnbs6Ohrt9uVkpKiNWvWuHVtJBIAAFjMW0MbpT3xOjMzs9RzJicna86cOfroo480ffp05ebmqnXr1jp48KByc3MlSTExMU7viYmJcexzFbd/AgBgMW+tI1HaE6/PfnDlGZ07d3b889VXX61WrVqpVq1amj17tq699tpS4zLGuB0rFQkAAC4TdrtdkZGRTtu5EomzhYeH6+qrr9b333/vuHvj7OrDvn37SlQpLoREAgAAi3lrjsTFKCgo0LZt2xQXF6fExETFxsZqxYoVjv0nT57UqlWr1Lp1a7eOy9AGAAAW88XKlo888oi6du2qhIQE7du3T//85z917NgxpaamymazKS0tTRkZGUpKSlJSUpIyMjIUFhamnj17unUeEgkAAPzQzz//rHvuuUcHDhzQlVdeqWuvvVbr1q1T9erVJUmjRo1Sfn6+hgwZosOHDys5OVnLly9XRESEW+exGWOMFRfgS91f3ejrEIAy6fU+zXwdAlDmhAVbXy5o+sRKrxxn07gbvXIcb6IiAQCAxXhoFwAAQCmoSAAAYDFvrSNRFpFIAABgMT/OIxjaAAAAnqMiAQCAxRjaAAAAHvPjPIJEAgAAq/lzRYI5EgAAwGNUJAAAsJgfFyRIJAAAsBpDGwAAAKWgIgEAgMX8uCBBIgEAgNUY2gAAACgFFQkAACzmxwUJEgkAAKzG0AYAAEApqEgAAGAxf65IkEgAAGAxP84jSCQAALCaP1ckmCMBAAA8RkUCAACL+XFBgkQCAACrMbQBAABQCioSAABYzI8LEiQSAABYrZwfZxIMbQAAAI9RkQAAwGJ+XJAgkQAAwGr+fNcGiQQAABYr5795BHMkAACA56hIAABgMYY2AACAx/w4j2BoAwAAeI6KBAAAFrPJf0sSJBIAAFjMn+/acCmReO+991w+4G233eZxMAAA4PLiUiLRrVs3lw5ms9lUVFR0MfEAAOB3/vJ3bRQXF1sdBwAAfsuP84iLu2vjxIkT3ooDAABchtxOJIqKijR+/HhVqVJF5cuX186dOyVJjz32mF599VWvBwgAwOWunM3mla0scjuReOqppzRr1ixlZWUpODjY0X711VfrlVde8WpwAAD4A5vNO1tZ5HYiMWfOHL388svq1auXAgICHO2NGjXS9u3bvRocAAD+wGazeWW7GJmZmbLZbEpLS3O0GWOUnp6u+Ph4hYaGqm3bttq6datbx3U7kfjll19Uu3btEu3FxcU6deqUu4cDAAAWy87O1ssvv6xGjRo5tWdlZWnixImaPHmysrOzFRsbqw4dOuj48eMuH9vtRKJBgwZavXp1ifa33npLTZs2dfdwAAD4PV8Obfz+++/q1auXpk+frgoVKjjajTGaNGmSxo4dq+7du6thw4aaPXu2/vjjD82dO9fl47u9suW4cePUp08f/fLLLyouLtbChQu1Y8cOzZkzR0uXLnX3cAAA+D1vTZQsKChQQUGBU5vdbpfdbj/ne4YOHaouXbropptu0j//+U9He05OjnJzc9WxY0enY6WkpGjNmjUaOHCgSzG5XZHo2rWr5s+frw8++EA2m02PP/64tm3bpiVLlqhDhw7uHg4AALgoMzNTUVFRTltmZuY5+8+bN09ff/11qX1yc3MlSTExMU7tMTExjn2u8OhZG506dVKnTp08eSsAAH853rrhYsyYMRoxYoRT27mqET/99JMefvhhLV++XCEhIeeO7axqiTHGrYmdHj+0a8OGDdq2bZtsNpvq1aun5s2be3ooAAD8mreWyL7QMMafbdy4Ufv27XP6/VxUVKTPP/9ckydP1o4dOySdrkzExcU5+uzbt69EleJ83E4kfv75Z91zzz368ssvdcUVV0iSjhw5otatW+vNN99UtWrV3D0kAADwsvbt22vLli1Obf369dNVV12l0aNHq2bNmoqNjdWKFSscN0ucPHlSq1at0oQJE1w+j9uJRP/+/XXq1Clt27ZNdevWlSTt2LFD/fv314ABA7R8+XJ3DwkAgF/zxWPEIyIi1LBhQ6e28PBwRUdHO9rT0tKUkZGhpKQkJSUlKSMjQ2FhYerZs6fL53E7kVi9erXWrFnjSCIkqW7dunrhhRd03XXXuXs4AAD8Xll9+ueoUaOUn5+vIUOG6PDhw0pOTtby5csVERHh8jHcTiQSEhJKXXiqsLBQVapUcfdwAADgEvnss8+cXttsNqWnpys9Pd3jY7p9+2dWVpYefPBBbdiwQcYYSacnXj788MN69tlnPQ4EAAB/5c/P2nCpIlGhQgWnskxeXp6Sk5MVGHj67YWFhQoMDFT//v3VrVs3SwIFAOByVVaHNrzBpURi0qRJFocBAID/8sVky0vFpUQiNTXV6jgAAMBlyOMFqSQpPz+/xMTLyMjIiwoIAAB/489DG25PtszLy9OwYcNUuXJllS9fXhUqVHDaAACAM5uXtrLI7URi1KhRWrlypaZMmSK73a5XXnlFTzzxhOLj4zVnzhwrYgQAAGWU20MbS5Ys0Zw5c9S2bVv1799fbdq0Ue3atVW9enW98cYb6tWrlxVxAgBw2fLWY8TLIrcrEocOHVJiYqKk0/MhDh06JEm6/vrr9fnnn3s3OgAA/IA/ryPhdiJRs2ZN7dq1S5JUv359LViwQNLpSsWZh3gBAIC/BrcTiX79+uk///mPpNPPRT8zV2L48OEaOXKk1wMEAOByZ7PZvLKVRW7PkRg+fLjjn9u1a6ft27drw4YNqlWrlho3buzV4AAA8AdlNAfwCrcrEmdLSEhQ9+7dVbFiRfXv398bMQEAgMvERScSZxw6dEizZ8/21uEAAPAb5Ww2r2xl0UWtbAkAAC6sjOYAXkEiAQCAxcrqRElv8NrQBgAA+OtxuSLRvXv38+4/cuTIxcbiNXNTm/s6BKBMqtBymK9DAMqc/E2TLT+HP//V7nIiERUVdcH9995770UHBACAv/HnoQ2XE4mZM2daGQcAALgMMdkSAACLlfPfggSJBAAAVvPnRMKf538AAACLUZEAAMBiTLYEAAAeY2jjLK+99pquu+46xcfHa/fu3ZKkSZMm6d133/VqcAAAoGxzO5GYOnWqRowYoVtuuUVHjhxRUVGRJOmKK67QpEmTvB0fAACXPZvNO1tZ5HYi8cILL2j69OkaO3asAgICHO0tWrTQli1bvBocAAD+gKd//klOTo6aNm1aot1utysvL88rQQEA4E/8+RZJt68tMTFRmzdvLtH+4Ycfqn79+t6ICQAAXCbcrkiMHDlSQ4cO1YkTJ2SM0fr16/Xmm28qMzNTr7zyihUxAgBwWSujoxJe4XYi0a9fPxUWFmrUqFH6448/1LNnT1WpUkXPP/+8evToYUWMAABc1srq/AZv8Ggdifvvv1/333+/Dhw4oOLiYlWuXNnbcQEAgMvARS1IValSJW/FAQCA3/LjgoT7iURiYuJ5l/rcuXPnRQUEAIC/8eeVLd1OJNLS0pxenzp1Sps2bdKyZcs0cuRIb8UFAAAuA24nEg8//HCp7S+++KI2bNhw0QEBAOBv/HmypdfWyOjcubPeeecdbx0OAAC/wRLZLnj77bdVsWJFbx0OAABcBtwe2mjatKnTZEtjjHJzc7V//35NmTLFq8EBAOAPmGz5J926dXN6Xa5cOV155ZVq27atrrrqKm/FBQCA37DJfzMJtxKJwsJC1ahRQ506dVJsbKxVMQEA4Ff8uSLh1hyJwMBADR48WAUFBVbFAwAAvGDq1Klq1KiRIiMjFRkZqVatWunDDz907DfGKD09XfHx8QoNDVXbtm21detWt8/j9mTL5ORkbdq0ye0TAQDwV1XO5p3NHVWrVtXTTz+tDRs2aMOGDbrxxht1++23O5KFrKwsTZw4UZMnT1Z2drZiY2PVoUMHHT9+3K3zuD1HYsiQIfr73/+un3/+Wc2bN1d4eLjT/kaNGrl7SAAA/Nr5VoS2SteuXZ1eP/XUU5o6darWrVun+vXra9KkSRo7dqy6d+8uSZo9e7ZiYmI0d+5cDRw40OXzuJxI9O/fX5MmTdLdd98tSXrooYcc+2w2m4wxstlsKioqcvnkAADAdQUFBSWmF9jtdtnt9vO+r6ioSG+99Zby8vLUqlUr5eTkKDc3Vx07dnQ6TkpKitasWeNWIuHy0Mbs2bN14sQJ5eTklNh27tzp+H8AAODMW0MbmZmZioqKctoyMzPPed4tW7aofPnystvtGjRokBYtWqT69esrNzdXkhQTE+PUPyYmxrHPVS5XJIwxkqTq1au7dQIAAP7qvDWyMWbMGI0YMcKp7XzViLp162rz5s06cuSI3nnnHaWmpmrVqlV/iss5sDOjC+5wa46EL8Z4AADAaa4MY/xZcHCwateuLUlq0aKFsrOz9fzzz2v06NGSpNzcXMXFxTn679u3r0SV4kLcSiTq1KlzwWTi0KFDbgUAAIC/KysP7TLGqKCgQImJiYqNjdWKFSvUtGlTSdLJkye1atUqTZgwwa1jupVIPPHEE4qKinLrBAAA/NX5YkGqRx99VJ07d1a1atV0/PhxzZs3T5999pmWLVsmm82mtLQ0ZWRkKCkpSUlJScrIyFBYWJh69uzp1nncSiR69OihypUru3UCAABw6f3222/q06eP9u7dq6ioKDVq1EjLli1Thw4dJEmjRo1Sfn6+hgwZosOHDys5OVnLly9XRESEW+exmTOzKC8gICBAe/fuvSwSiROFvo4AKJsqtBzm6xCAMid/02TLz/HClzleOc6D1yV65Tje5PZdGwAAwD3leGiXVFxcbGUcAAD4rTIy19ISbj9rAwAA4Ay3n7UBAADc48+PESeRAADAYmVlHQkrMLQBAAA8RkUCAACL+XFBgkQCAACrMbQBAABQCioSAABYzI8LEiQSAABYzZ/L//58bQAAwGJUJAAAsJjNj8c2SCQAALCY/6YRJBIAAFiO2z8BAABKQUUCAACL+W89gkQCAADL+fHIBkMbAADAc1QkAACwGLd/AgAAj/lz+d+frw0AAFiMigQAABZjaAMAAHjMf9MIhjYAAMBFoCIBAIDFGNoAAAAe8+fyP4kEAAAW8+eKhD8nSQAAwGJUJAAAsJj/1iNIJAAAsJwfj2wwtAEAADxHRQIAAIuV8+PBDRIJAAAsxtAGAABAKahIAABgMRtDGwAAwFMMbQAAAJSCigQAABbjrg0AAOAxfx7aIJEAAMBi/pxIMEcCAAB4jIoEAAAW8+fbP6lIAABgsXI272zuyMzMVMuWLRUREaHKlSurW7du2rFjh1MfY4zS09MVHx+v0NBQtW3bVlu3bnXv2twLCwAAXA5WrVqloUOHat26dVqxYoUKCwvVsWNH5eXlOfpkZWVp4sSJmjx5srKzsxUbG6sOHTro+PHjLp/HZowxVlyAL50o9HUEQNlUoeUwX4cAlDn5myZbfo6V2w965Tg3XhXt8Xv379+vypUra9WqVbrhhhtkjFF8fLzS0tI0evRoSVJBQYFiYmI0YcIEDRw40KXjUpEAAMBiNpt3toKCAh07dsxpKygocCmGo0ePSpIqVqwoScrJyVFubq46duzo6GO325WSkqI1a9a4fG0kEgAAXCYyMzMVFRXltGVmZl7wfcYYjRgxQtdff70aNmwoScrNzZUkxcTEOPWNiYlx7HMFd20AAGAxb921MWbMGI0YMcKpzW63X/B9w4YN0zfffKMvvviiZGxnLXJhjCnRdj4kEgAAWMzdOy7OxW63u5Q4/NmDDz6o9957T59//rmqVq3qaI+NjZV0ujIRFxfnaN+3b1+JKsX5MLQBAIAfMsZo2LBhWrhwoVauXKnExESn/YmJiYqNjdWKFSscbSdPntSqVavUunVrl89DRQKW2LghW7NmvKpt332r/fv367l/v6gb29/k67CAS2b7+0+oenzJGfbT5n+u4U8vkCSNHXiLBvzfdboiIlTZ3+5WWuZ8bdvp+tg0Lh++WJBq6NChmjt3rt59911FREQ45j1ERUUpNDRUNptNaWlpysjIUFJSkpKSkpSRkaGwsDD17NnT5fOQSMAS+fl/qG7durr9b93197QHfR0OcMld3/sZBfypnl2/drw+mPagFq7YJEn6e9+b9FDvdnpg3Ov6fvc+/b/7b9b70x5Uo25P6vc/XJuFj8uHL561MXXqVElS27Ztndpnzpypvn37SpJGjRql/Px8DRkyRIcPH1ZycrKWL1+uiIgIl89DIgFLXN8mRde3SfF1GIDPHDj8u9PrR/o11I979mv1xu8lSUN7tlPWqx/p3ZX/kSTd99hr2v1Jhu7u3EKvvvPlJY8X1vLFAtmuLBNls9mUnp6u9PR0j8/DHAkAsFhQYIB63NJSs99dK0mqUSVacVdG6eO12x19Tp4q1OqNP+jaxjV9FSbgkTKdSPz000/q37//eftczOIcAHAp3Nauka6ICNXrS76SJMVWipQk7TvkvAzxvoPHFRMdecnjg/XK2Wxe2cqiMp1IHDp0SLNnzz5vn9IW53hmwoUX5wCASyW1W2t99OV32rv/qFP72aVnm821cjQuPzYvbWWRT+dIvPfee+fdv3Pnzgseo7TFOUyAe/fYAoBVEuIq6MbkuurxyHRHW+6BY5KkmOhIxz9L0pUVI0pUKYCyzqeJRLdu3WSz2c6bgV9oda3SFufgoV0Ayoo+t7XSvkPH9eHq/z2aedcvB7V3/1G1v/Yq/WfHz5JOz6No07y2/vH8u74KFVYqq+UEL/Dp0EZcXJzeeecdFRcXl7p9/fXXvgwPF+GPvDxt37ZN27dtkyT98vPP2r5tm/b++quPIwMuHZvNpntvv1ZvLP1KRUXFTvtenPupRg7oqNvaNVL9WnGa/mQf5Z84pfkfbvBRtLCSzUv/K4t8WpFo3ry5vv76a3Xr1q3U/ReqVqDs2rr1W93X717H62ezTs9bue32v2l8xtO+Cgu4pG5MrquEuIqavXhdiX3/mvWxQuzBmjTmblWIDFP2t7t06+DJrCGBy47N+PA39erVq5WXl6ebb7651P15eXnasGGDUlLcW4+AoQ2gdBVaDvN1CECZk79psuXnWL/z6IU7ueCamlFeOY43+bQi0aZNm/PuDw8PdzuJAACgrCmbgxLeUaZv/wQAAGUbS2QDAGA1Py5JkEgAAGCxsnrHhTeQSAAAYLEyurq1VzBHAgAAeIyKBAAAFvPjggSJBAAAlvPjTIKhDQAA4DEqEgAAWIy7NgAAgMe4awMAAKAUVCQAALCYHxckSCQAALCcH2cSDG0AAACPUZEAAMBi3LUBAAA85s93bZBIAABgMT/OI5gjAQAAPEdFAgAAq/lxSYJEAgAAi/nzZEuGNgAAgMeoSAAAYDHu2gAAAB7z4zyCoQ0AAOA5KhIAAFjNj0sSJBIAAFiMuzYAAABKQUUCAACLcdcGAADwmB/nESQSAABYzo8zCeZIAAAAj1GRAADAYv581waJBAAAFvPnyZYMbQAA4Kc+//xzde3aVfHx8bLZbFq8eLHTfmOM0tPTFR8fr9DQULVt21Zbt2516xwkEgAAWMzmpc1deXl5aty4sSZPnlzq/qysLE2cOFGTJ09Wdna2YmNj1aFDBx0/ftzlczC0AQCA1Xw0tNG5c2d17ty51H3GGE2aNEljx45V9+7dJUmzZ89WTEyM5s6dq4EDB7p0DioSAAD8BeXk5Cg3N1cdO3Z0tNntdqWkpGjNmjUuH4eKBAAAFvPWXRsFBQUqKChwarPb7bLb7W4fKzc3V5IUExPj1B4TE6Pdu3e7fBwqEgAAWMxm886WmZmpqKgopy0zM/MiY3NOcowxJdrOh4oEAACXiTFjxmjEiBFObZ5UIyQpNjZW0unKRFxcnKN93759JaoU50NFAgAAi3nrrg273a7IyEinzdNEIjExUbGxsVqxYoWj7eTJk1q1apVat27t8nGoSAAAYDUf3bXx+++/64cffnC8zsnJ0ebNm1WxYkUlJCQoLS1NGRkZSkpKUlJSkjIyMhQWFqaePXu6fA4SCQAALOarJbI3bNigdu3aOV6fGRZJTU3VrFmzNGrUKOXn52vIkCE6fPiwkpOTtXz5ckVERLh8Dpsxxng9ch87UejrCICyqULLYb4OAShz8jeVvliTN+0+WHDhTi6oHu3ZMIaVqEgAAGAxf37WBokEAAAW8+M8grs2AACA56hIAABgMYY2AADARfDfTIKhDQAA4DEqEgAAWIyhDQAA4DE/ziMY2gAAAJ6jIgEAgMUY2gAAAB7z1bM2LgUSCQAArOa/eQRzJAAAgOeoSAAAYDE/LkiQSAAAYDV/nmzJ0AYAAPAYFQkAACzGXRsAAMBz/ptHMLQBAAA8R0UCAACL+XFBgkQCAACrcdcGAABAKahIAABgMe7aAAAAHmNoAwAAoBQkEgAAwGMMbQAAYDF/HtogkQAAwGL+PNmSoQ0AAOAxKhIAAFiMoQ0AAOAxP84jGNoAAACeoyIBAIDV/LgkQSIBAIDFuGsDAACgFFQkAACwGHdtAAAAj/lxHkEiAQCA5fw4k2COBAAA8BgVCQAALObPd22QSAAAYDF/nmzJ0AYAAPCYzRhjfB0E/FNBQYEyMzM1ZswY2e12X4cDlBn8bMCfkEjAMseOHVNUVJSOHj2qyMhIX4cDlBn8bMCfMLQBAAA8RiIBAAA8RiIBAAA8RiIBy9jtdo0bN47JZMBZ+NmAP2GyJQAA8BgVCQAA4DESCQAA4DESCQAA4DESCQAA4DESCVhmypQpSkxMVEhIiJo3b67Vq1f7OiTApz7//HN17dpV8fHxstlsWrx4sa9DAi4aiQQsMX/+fKWlpWns2LHatGmT2rRpo86dO2vPnj2+Dg3wmby8PDVu3FiTJ0/2dSiA13D7JyyRnJysZs2aaerUqY62evXqqVu3bsrMzPRhZEDZYLPZtGjRInXr1s3XoQAXhYoEvO7kyZPauHGjOnbs6NTesWNHrVmzxkdRAQCsQCIBrztw4ICKiooUExPj1B4TE6Pc3FwfRQUAsAKJBCxjs9mcXhtjSrQBAC5vJBLwukqVKikgIKBE9WHfvn0lqhQAgMsbiQS8Ljg4WM2bN9eKFSuc2lesWKHWrVv7KCoAgBUCfR0A/NOIESPUp08ftWjRQq1atdLLL7+sPXv2aNCgQb4ODfCZ33//XT/88IPjdU5OjjZv3qyKFSsqISHBh5EBnuP2T1hmypQpysrK0t69e9WwYUM999xzuuGGG3wdFuAzn332mdq1a1eiPTU1VbNmzbr0AQFeQCIBAAA8xhwJAADgMRIJAADgMRIJAADgMRIJAADgMRIJAADgMRIJAADgMRIJAADgMRIJoAxIT09XkyZNHK/79u2rbt26XfI4du3aJZvNps2bN1t2jrOv1ROXIk4AriGRAM6hb9++stlsstlsCgoKUs2aNfXII48oLy/P8nM///zzLq90eKl/qbZt21ZpaWmX5FwAyj6etQGcx80336yZM2fq1KlTWr16te677z7l5eVp6tSpJfqeOnVKQUFBXjlvVFSUV44DAFajIgGch91uV2xsrKpVq6aePXuqV69eWrx4saT/lehnzJihmjVrym63yxijo0eP6oEHHlDlypUVGRmpG2+8Uf/5z3+cjvv0008rJiZGERERGjBggE6cOOG0/+yhjeLiYk2YMEG1a9eW3W5XQkKCnnrqKUlSYmKiJKlp06ay2Wxq27at430zZ85UvXr1FBISoquuukpTpkxxOs/69evVtGlThYSEqEWLFtq0adNFf2ajR49WnTp1FBYWppo1a+qxxx7TqVOnSvR76aWXVK1aNYWFhenOO+/UkSNHnPZfKHYAZQMVCcANoaGhTr8Uf/jhBy1YsEDvvPOOAgICJEldunRRxYoV9cEHHygqKkovvfSS2rdvr//+97+qWLGiFixYoHHjxunFF19UmzZt9Nprr+nf//63atasec7zjhkzRtOnT9dzzz2n66+/Xnv37tX27dslnU4GrrnmGn388cdq0KCBgoODJUnTp0/XuHHjNHnyZDVt2lSbNm3S/fffr/DwcKWmpiovL0+33nqrbrzxRr3++uvKycnRww8/fNGfUUREhGbNmqX4+Hht2bJF999/vyIiIjRq1KgSn9uSJUt07NgxDRgwQEOHDtUbb7zhUuwAyhADoFSpqanm9ttvd7z+6quvTHR0tLnrrruMMcaMGzfOBAUFmX379jn6fPLJJyYyMtKcOHHC6Vi1atUyL730kjHGmFatWplBgwY57U9OTjaNGzcu9dzHjh0zdrvdTJ8+vdQ4c3JyjCSzadMmp/Zq1aqZuXPnOrWNHz/etGrVyhhjzEsvvWQqVqxo8vLyHPunTp1a6rH+LCUlxTz88MPn3H+2rKws07x5c8frcePGmYCAAPPTTz852j788ENTrlw5s3fvXpdiP9c1A7j0qEgA57F06VKVL19ehYWFOnXqlG6//Xa98MILjv3Vq1fXlVde6Xi9ceNG/f7774qOjnY6Tn5+vn788UdJ0rZt2zRo0CCn/a1atdKnn35aagzbtm1TQUGB2rdv73Lc+/fv108//aQBAwbo/vvvd7QXFhY65l9s27ZNjRs3VlhYmFMcF+vtt9/WpEmT9MMPP+j3339XYWGhIiMjnfokJCSoatWqTuctLi7Wjh07FBAQcMHYAZQdJBLAebRr105Tp05VUFCQ4uPjS0ymDA8Pd3pdXFysuLg4ffbZZyWOdcUVV3gUQ2hoqNvvKS4ulnR6iCA5Odlp35khGGOMR/Gcz7p169SjRw898cQT6tSpk6KiojRv3jz961//Ou/7bDab4/9diR1A2UEiAZxHeHi4ateu7XL/Zs2aKTc3V4GBgapRo0apferVq6d169bp3nvvdbStW7funMdMSkpSaGioPvnkE913330l9p+ZE1FUVORoi4mJUZUqVbRz50716tWr1OPWr19fr732mvLz8x3JyvnicMWXX36p6tWra+zYsY623bt3l+i3Z88e/frrr4qPj5ckrV27VuXKlVOdOnVcih1A2UEiAXjRTTfdpFatWqlbt26aMGGC6tatq19//VUffPCBunXrphYtWujhhx9WamqqWrRooeuvv15vvPGGtm7des7JliEhIRo9erRGjRql4OBgXXfdddq/f7+2bt2qAQMGqHLlygoNDdWyZctUtWpVhYSEKCoqSunp6XrooYcUGRmpzp07q6CgQBs2bNDhw4c1YsQI9ezZU2PHjtWAAQP0j3/8Q7t27dKzzz7r0nXu37+/xLoVsbGxql27tvbs2aN58+apZcuWev/997Vo0aJSryk1NVXPPvusjh07poceekh33XWXYmNjJemCsQMoQ3w9SQMoq86ebHm2cePGOU2QPOPYsWPmwQcfNPHx8SYoKMhUq1bN9OrVy+zZs8fR56mnnjKVKlUy5cuXN6mpqWbUqFHnnGxpjDFFRUXmn//8p6levboJCgoyCQkJJiMjw7F/+vTpplq1aqZcuXImJSXF0f7GG2+YJk2amODgYFOhQgVzww03mIULFzr2r1271jRu3NgEBwebJk2amHfeecelyZaSSmzjxo0zxhgzcuRIEx0dbcqXL2/uvvtu89xzz5moqKgSn9uUKVNMfHy8CQkJMd27dzeHDh1yOs/5YmeyJVB22IyxYKAUAAD8JbAgFQAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8BiJBAAA8Nj/B5rAA+aVVogaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train CatBoost Classifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    verbose=False\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2131d3-13fe-4954-81bd-1eb7a2bfc735",
   "metadata": {},
   "source": [
    "## Question 10: You're working for a FinTech company trying to predict loan default using\n",
    "customer demographics and transaction behavior.\n",
    "The dataset is imbalanced, contains missing values, and has both numeric and\n",
    "categorical features.\n",
    "Describe your step-by-step data science pipeline using boosting techniques:\n",
    "● Data preprocessing & handling missing/categorical values\n",
    "● Choice between AdaBoost, XGBoost, or CatBoost\n",
    "● Hyperparameter tuning strategy\n",
    "● Evaluation metrics you'd choose and why\n",
    "● How the business would benefit from your model\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a4d26-e236-46a3-9ae6-44c3378623f0",
   "metadata": {},
   "source": [
    "### Answer:Step-by-Step Data Science Pipeline\n",
    "\n",
    "**Use case:** Loan Default Prediction (Imbalanced + Missing + Mixed data)\n",
    "\n",
    "## 1️.Data Preprocessing & Handling Issues\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "* **Numeric features** → fill with **median** (robust to outliers)\n",
    "* **Categorical features** → fill with **“Unknown”**\n",
    "* Why? Keeps data size intact (important for imbalanced data)\n",
    "\n",
    "### Categorical Variables\n",
    "\n",
    "* **Best choice:** CatBoost\n",
    "\n",
    "  * Handles categorical features **natively**\n",
    "  * No need for One-Hot Encoding\n",
    "* If using XGBoost → need encoding (OneHot / Target Encoding)\n",
    "\n",
    "### Imbalanced Data\n",
    "\n",
    "* Default cases are **minority class**\n",
    "* Use:\n",
    "\n",
    "  * `class_weight='balanced'` (CatBoost / AdaBoost)\n",
    "  * OR `scale_pos_weight` (XGBoost)\n",
    "\n",
    "\n",
    "## 2️.Choice of Boosting Algorithm (Most Important)\n",
    "\n",
    "### **CatBoost (Best choice here)**\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* Handles **missing values automatically**\n",
    "* Handles **categorical features directly**\n",
    "* Works very well on **imbalanced datasets**\n",
    "* Minimal preprocessing → less error-prone\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "* Weak with noisy + missing data\n",
    "\n",
    "###  XGBoost\n",
    "\n",
    "* Very powerful, but:\n",
    "\n",
    "  * Needs heavy preprocessing\n",
    "  * Manual encoding required\n",
    "\n",
    "**Final choice:** **CatBoost**\n",
    "\n",
    "## 3️. Hyperparameter Tuning Strategy\n",
    "\n",
    "Use **GridSearchCV** or **RandomizedSearchCV** to tune:\n",
    "\n",
    "* `learning_rate` → controls step size\n",
    "* `depth` → controls model complexity\n",
    "* `iterations` → number of trees\n",
    "\n",
    "Use **cross-validation (cv=5)** to avoid overfitting.\n",
    "\n",
    "## 4️. Evaluation Metrics\n",
    "\n",
    "Accuracy is  misleading (because data is imbalanced).\n",
    "\n",
    "### Best metrics:\n",
    "\n",
    "* **Recall** → catch maximum defaulters (business critical)\n",
    "* **Precision** → avoid false alarms\n",
    "* **F1-score** → balance between precision & recall\n",
    "* **ROC-AUC** → overall discrimination power\n",
    "\n",
    "**Most important for FinTech:**\n",
    "**Recall + ROC-AUC**\n",
    "\n",
    "## 5️. Business Benefits\n",
    "\n",
    "*  Early identification of **high-risk customers**\n",
    "*  Reduced **loan default losses**\n",
    "*  Better **credit risk decisions**\n",
    "*  Improved **profitability & trust**\n",
    "*  Regulatory compliance support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e639d0-372d-4264-a95c-f56d2334aea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_9060\\4235087336.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['age'].fillna(X['age'].median(), inplace=True)\n",
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_9060\\4235087336.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['income'].fillna(X['income'].median(), inplace=True)\n",
      "C:\\Users\\khush\\AppData\\Local\\Temp\\ipykernel_9060\\4235087336.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X['gender'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "ROC-AUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Dummy loan default dataset (example)\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, 45, None, 35, 50, 23, None, 60],\n",
    "    'income': [50000, 80000, 30000, None, 90000, 40000, 70000, None],\n",
    "    'gender': ['M', 'F', 'M', 'F', None, 'M', 'F', 'M'],\n",
    "    'loan_default': [0, 1, 0, 1, 0, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Separate features & target\n",
    "X = data.drop('loan_default', axis=1)\n",
    "y = data['loan_default']\n",
    "\n",
    "# Fill missing values (safe step)\n",
    "X['age'].fillna(X['age'].median(), inplace=True)\n",
    "X['income'].fillna(X['income'].median(), inplace=True)\n",
    "X['gender'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Categorical feature index (NOT name)\n",
    "cat_features = [X.columns.get_loc('gender')]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# CatBoost Classifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    class_weights=[1, 3],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, cat_features=cat_features)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
