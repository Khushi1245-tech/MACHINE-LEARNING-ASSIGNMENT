{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1a3a2f-7c8c-4a52-b15f-7b25da33ec9e",
   "metadata": {},
   "source": [
    "# **KNN & PCA Assignment**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227ef23-ed69-42d5-8b7a-bd1be37b7b0b",
   "metadata": {},
   "source": [
    "## Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
    "\n",
    "### Answer:K-Nearest Neighbors (KNN) is a simple and intuitive supervised machine learning algorithm that works on the principle of similarity. Instead of building an explicit model during training, KNN stores the entire training dataset and makes predictions at the time of testing. When a new data point is given, the algorithm calculates the distance between this point and all training data points using a distance measure such as Euclidean distance. It then selects the K closest data points, known as neighbors. In classification problems, KNN predicts the class of the new data point by taking a majority vote among the classes of these nearest neighbors, while in regression problems, it predicts a continuous value by calculating the average (or weighted average) of the target values of the nearest neighbors. Thus, KNN uses the same basic mechanism for both tasks, differing only in how the final prediction is made—voting for classification and averaging for regression.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc01a33-3d57-4882-97ed-1630bb4cb5fc",
   "metadata": {},
   "source": [
    "## Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
    "\n",
    "### Answer:The **curse of dimensionality** refers to the problems that arise when the number of features (dimensions) in a dataset becomes very large. As dimensionality increases, the data points become more sparse in the feature space, and the distance between points starts to lose its meaning. In the context of K-Nearest Neighbors (KNN), this directly affects performance because KNN relies heavily on distance calculations to find similar data points. When there are too many dimensions, the distances between the nearest and farthest neighbors become almost the same, making it difficult for KNN to correctly identify truly “nearest” neighbors. As a result, KNN becomes less accurate, more sensitive to noise, and computationally expensive. Therefore, high dimensional data can significantly degrade KNN’s performance unless dimensionality reduction or feature selection techniques are applied.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141e725-50fe-402f-a83d-d2394f0102d5",
   "metadata": {},
   "source": [
    "## Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
    "\n",
    "### Answer: Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much variance (information) as possible. It works by transforming the original correlated features into a new set of uncorrelated variables called principal components, which are ordered such that the first few components capture the maximum variance in the data. PCA changes the original feature space by creating new features that are linear combinations of the original ones, and it does not consider the target variable while performing this transformation.\n",
    "\n",
    "### PCA is different from feature selection because feature selection does not create new features; instead, it selects a subset of the most important original features based on certain criteria such as correlation, statistical tests, or model-based importance. While PCA focuses on reducing dimensionality by transforming features, feature selection focuses on retaining interpretability by keeping the original features intact.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696313c5-7f91-4150-a4b1-000f109ac14b",
   "metadata": {},
   "source": [
    "## Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
    "\n",
    "### Answer:In Principal Component Analysis (PCA), **eigenvectors and eigenvalues** are mathematical concepts derived from the **covariance matrix** of the dataset and play a central role in dimensionality reduction. **Eigenvectors** represent the directions (principal components) along which the data varies the most, while **eigenvalues** indicate the amount of variance captured along each of those directions. A larger eigenvalue means that its corresponding eigenvector explains more information present in the data. PCA ranks eigenvectors in descending order of their eigenvalues and selects the top ones to form a lower-dimensional representation of the data. These selected eigenvectors ensure that maximum variance is retained while reducing dimensions, which is why eigenvalues and eigenvectors are crucial for identifying the most informative directions in the data.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91150afd-11f7-4f52-af87-6b1bf82e656e",
   "metadata": {},
   "source": [
    "## Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
    "\n",
    "### Answer:**How KNN and PCA complement each other in a single pipeline:**\n",
    "\n",
    "* PCA reduces the number of features by transforming high-dimensional data into a lower-dimensional space.\n",
    "* By reducing dimensions, PCA helps overcome the **curse of dimensionality**, which negatively affects KNN.\n",
    "* KNN relies on distance calculations, and PCA makes these distances more meaningful by removing noise and redundant features.\n",
    "* PCA improves computational efficiency by reducing the number of distance calculations required by KNN.\n",
    "* Applying PCA before KNN often improves **accuracy and generalization** of the KNN model.\n",
    "* PCA helps KNN perform better on datasets with many correlated features.\n",
    "* Together, PCA (as preprocessing) and KNN (as a classifier/regressor) form an efficient and effective machine learning pipeline.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab97e4d-667f-46a1-9c47-ba70195147a4",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
    "## Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc047d9-5c19-44c9-8735-8a58492d06eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without feature scaling: 0.7222222222222222\n",
      "Accuracy with feature scaling: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------- KNN WITHOUT FEATURE SCALING -----------\n",
    "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_scaling.fit(X_train, y_train)\n",
    "\n",
    "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
    "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
    "\n",
    "print(\"Accuracy without feature scaling:\", accuracy_no_scaling)\n",
    "\n",
    "# ----------- KNN WITH FEATURE SCALING -----------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(\"Accuracy with feature scaling:\", accuracy_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc58cb-2926-4891-91f3-b79a907f9370",
   "metadata": {},
   "source": [
    "## Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a71be6-0cac-4a14-a318-2d113aa576e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Component 1: 0.3620\n",
      "Principal Component 2: 0.1921\n",
      "Principal Component 3: 0.1112\n",
      "Principal Component 4: 0.0707\n",
      "Principal Component 5: 0.0656\n",
      "Principal Component 6: 0.0494\n",
      "Principal Component 7: 0.0424\n",
      "Principal Component 8: 0.0268\n",
      "Principal Component 9: 0.0222\n",
      "Principal Component 10: 0.0193\n",
      "Principal Component 11: 0.0174\n",
      "Principal Component 12: 0.0130\n",
      "Principal Component 13: 0.0080\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "for i, var in enumerate(explained_variance, start=1):\n",
    "    print(f\"Principal Component {i}: {var:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bec941-c20a-4c96-8589-ec3acc40965a",
   "metadata": {},
   "source": [
    "## Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42223da5-9efc-44ca-91a4-2ef5b5738707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original dataset: 0.9444444444444444\n",
      "Accuracy on PCA-transformed dataset: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------- KNN on ORIGINAL DATA (with scaling) -----------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_original.fit(X_train_scaled, y_train)\n",
    "y_pred_original = knn_original.predict(X_test_scaled)\n",
    "\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "print(\"Accuracy on original dataset:\", accuracy_original)\n",
    "\n",
    "# ----------- KNN on PCA-TRANSFORMED DATA (Top 2 components) -----------\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "\n",
    "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Accuracy on PCA-transformed dataset:\", accuracy_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b9384-5427-442b-ac95-5052057baa73",
   "metadata": {},
   "source": [
    "## Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a92f67-c5bf-47c0-ba20-1a828d5278ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (Euclidean): 0.9444\n",
      "KNN Accuracy (Manhattan): 0.9444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# 2. Split into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Scale the data (Crucial for distance-based KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Train and Evaluate KNN with Euclidean distance (p=2)\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train_scaled, y_train)\n",
    "y_pred_euc = knn_euclidean.predict(X_test_scaled)\n",
    "acc_euc = accuracy_score(y_test, y_pred_euc)\n",
    "\n",
    "# 5. Train and Evaluate KNN with Manhattan distance (p=1)\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_man = knn_manhattan.predict(X_test_scaled)\n",
    "acc_man = accuracy_score(y_test, y_pred_man)\n",
    "\n",
    "# Output Results\n",
    "print(f\"KNN Accuracy (Euclidean): {acc_euc:.4f}\")\n",
    "print(f\"KNN Accuracy (Manhattan): {acc_man:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ababbc9-f350-441b-9bde-dda7a77fbdcd",
   "metadata": {},
   "source": [
    "## Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
    "Due to the large number of features and a small number of samples, traditional models\n",
    "overfit.\n",
    "Explain how you would:\n",
    "● Use PCA to reduce dimensionality\n",
    "● Decide how many components to keep\n",
    "● Use KNN for classification post-dimensionality reduction\n",
    "● Evaluate the model\n",
    "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
    "biomedical data\n",
    "(Include your Python code and output in the code box below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44809ea0-7b11-4993-bb05-215f9bde60a7",
   "metadata": {},
   "source": [
    "\n",
    "### Answer: 1️. Using PCA to Reduce Dimensionality\n",
    "\n",
    "In gene expression data:\n",
    "\n",
    "* Features (genes) ≫ Samples (patients)\n",
    "* Many genes are correlated and noisy\n",
    "* High dimensionality causes **overfitting** and poor generalization\n",
    "\n",
    "**PCA (Principal Component Analysis)**:\n",
    "\n",
    "* Transforms original features into a smaller set of **orthogonal components**\n",
    "* Keeps maximum variance (biological signal)\n",
    "* Removes noise and redundancy\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Standardize features (important for PCA)\n",
    "2. Apply PCA\n",
    "3. Project data into lower-dimensional space\n",
    "\n",
    "## 2️. Deciding How Many Components to Keep\n",
    "\n",
    "We use **Explained Variance Ratio**:\n",
    "\n",
    "* Choose the minimum number of components that explain **90–95% variance**\n",
    "* This balances information retention and dimensionality reduction\n",
    "\n",
    "Methods:\n",
    "\n",
    "* Cumulative explained variance plot\n",
    "* `n_components=0.95` in PCA (automatic selection)\n",
    "\n",
    "## 3️. Using KNN After PCA\n",
    "\n",
    "Why **KNN + PCA** works well here:\n",
    "\n",
    "* KNN suffers in high dimensions (curse of dimensionality)\n",
    "* PCA creates compact, informative feature space\n",
    "* Distance calculations become meaningful\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "* PCA → KNN\n",
    "* Use small `k` (e.g., 3–7) to avoid over-smoothing\n",
    "\n",
    "## 4️. Model Evaluation\n",
    "\n",
    "We evaluate using:\n",
    "\n",
    "* **Accuracy**\n",
    "* **Confusion Matrix**\n",
    "* **Classification Report**\n",
    "* **Cross-validation (optional but recommended)**\n",
    "\n",
    "## 5️. Justifying This Pipeline to Stakeholders (Biomedical Context)\n",
    "\n",
    "**Why this is robust for real-world biomedical data:**\n",
    "\n",
    "* Reduces overfitting with small sample sizes\n",
    "* Improves interpretability by capturing dominant biological patterns\n",
    "* Distance-based KNN becomes reliable after PCA\n",
    "* Computationally efficient and reproducible\n",
    "* Commonly used in genomics and bioinformatics research\n",
    "\n",
    "> *“This pipeline balances biological signal preservation with statistical robustness, making it suitable for high-dimensional, low-sample biomedical datasets.”*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e046f6b-be51-49ab-9281-64416934c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 30\n",
      "Reduced features after PCA: 10\n",
      "\n",
      "Accuracy: 0.956140350877193\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        43\n",
      "           1       0.96      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset (proxy for gene expression data)\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"Original features:\", X.shape[1])\n",
    "print(\"Reduced features after PCA:\", X_train_pca.shape[1])\n",
    "\n",
    "# KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e163d1-aba6-4b44-97ee-09854aac8493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
